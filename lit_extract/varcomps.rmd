---
title: "read slopes"
author: "Jens Joschinski"
date: "March 15, 2018"
output: 
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
library(tidyr)
library(rjags)
```


# General description  
## Project aim  
To understand the evolvability of phenological strategies such as plasticity and bet-hedging, we correlate reaction norm properties with climate parameters. 

## Script overview  
We searched the Web of Science database for photoperiodic response curves of arthropods and selected 57 studies with individual reaction norms (dose-response curves) for 402 populations (447 reaction norms). The data was extracted with webplotdigitizer, and can be found in "lit_search/search_2018_11.xlsx".
This script does an MCMC to get estimates fo slopes, midpoints and upper and lower limit. It then calculates the variance within and between datapoints based on the same MCMC approach.


# Script  
## load data  

This chunk loads the extracted raw data from the 57 studies. It requires a .csv copy of the file "lit_search/search_2018_11.xlsx". First we calculate the number of populations, sample sizes etc. Then the diapause percentages (y-axis of the reaction norm) are rounded and converted in number of successes vs number of trials.
```{r load_data}
data<-read.table("lit_extract/variance_comp.csv", header=T,sep=";")
length(unique(data$ID)) #57 studies that can be used for slope calculation
length(unique(data$spec)) #45 species
length(unique(data$genus)) #32 genera
data<-unite(data,"popid",c("ID","pop","genus","spec"),sep=".",remove=F)
(number <- length(unique(data$popid))) #447 reaction norms


#summary of #pops per region and level of detail for sample sizes

t<-data.frame(unique(data$popid))
t$reg<-NA
t$nMethod<-NA
#each unique pop has multiple rows in dataset (1 per day length)
#get this subset, take region and nMethod from first line (or use unique)
for ( i in 1:nrow(t)){ 
   t$reg[i]<-unique(data[data$popid==t[i,1],5])
   t$nMethod[i]<-unique(data[data$popid==t[i,1],9])
}
table(t$reg) #unfortunately, the information was converted to values
levels(data$region)
#Asia 16
#Europe (including caucasus): 117
#Japan 256
#US 58
#japan has 57.3% of the populations, 47.8 % of the data


table(t$nMethod)
levels(data$nmethod)

#accurate: 97
#global average:291
#pop level: 26
#NA 33


#check whether some studies have <3 pops
t<-data.frame(unique(data$ID))
t$pop<-NA
for ( i in 1:nrow(t)){ 
  t$pop[i]<-length(unique(data[data$ID==t[i,1],1]))
}
#table t$pop

data$n2[is.na(data$n2)]<-100


data$perc[data$perc<0]<-0 #sometimes percentages are estimated as e.g. -0.3% or 101%. these are rounded here to prevent errors
data$perc[data$perc>100]<-100
data$n2<-round(data$n2)
data$number<-(data$perc/100) * data$n2
data$number<-floor(data$number)
number<-length(unique(data$popid))#447
data$n<-data$number/data$n2

#check whether some studies have <4 dls
#sort (table(data$popid))
#get a vector of form "ID-population" for later labelling
nams<-unite(data,"nams",c("ID","pop"), sep = "-", remove=T)$nams 

```



## MCMC  

Markov Chain Monte Carlo simulation. The estimates will be calculated independently for each of the 447 reaction norms. The actual model is in BUGS code, and interpreted by the R package rjags. Data is saved as .rds objects, so this chunk does not need to be repeated if changes are made to remainder of the script.
```{r jags}
#instead of saving as separate file this is put into a string, and later opened with textConnection(model_string)

model.string <- 
"model {
  
  #likelihood function
	for (i in 1:N){
		y[i] ~ dbin(success[i],trials[i])     #this incorporates sample size per point (number of individuals per day length)
		success[i] <- (d-c)/(1+exp(b*(x[i]-e)))+c #4-parameter logit curve
	}

	#priors
	b ~dunif(-100,100)       
	c ~dunif(0,1)
	d ~dunif(c,1)             #constrained to be bigger than c
	e ~dunif(min(x),max(x))   #constrained within range(day lengths)

}"


iter<-10000 #+1000 as burn-in
chains <-4

for (dataset  in (1:number)) {
  currpop <- unique(data$popid)[dataset]
  currdat<-data[data$popid==currpop,]
  
  
  model <- jags.model(textConnection(model.string), 
                    data = list(y=currdat$number, x= currdat$dl2, N = nrow(currdat),trials = currdat$n2), 
                    n.chains=chains, n.adapt=1000)

  out <- coda.samples(model,   c( 'b', 'c', 'd', 'e'), iter)
  outname <- paste("model_", currpop, sep="")
  assign(outname, out)
  save(list = outname ,file = paste(outname, ".rds", sep = ""))
  rm(list= outname)
}

```


## calculation of variance components  
We follow the trace of the MCMC (which was saved in previous step), and calculate the variance components at each iteration. The frequency distribution of variance estimates is used to get estimate  + credible interval.

```{r calc_variance}

logcurve<-function(x,b,c,d,e){(d-c)/(1+exp(b*(x-e)))+c} #4-paramter dose-response curve, eq.1

calc_var <- function(row){
  b<-row[1]
  c<-row[2]
  d<-row[3]
  e<-row[4]
  
  x<-seq(e-12,e+12, length.out =1000)
  y<-(d-c)/(1+exp(b*(x-e)))+c #produces logitcurve along a segment of mean +-12 h
  w <- sum(y*(1-y))/1000 #variance within, eq.2
  b <- sd(y)^2  #variance among, eq.3
  c(w, b,  b+w, b/(b+w)) #returns variance within, variance among, their sum (eq.5), and the ratio (eq.4)
}


for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  outname <- paste("model_", currpop, sep="")
  load(paste(outname, ".rds", sep=""))
  sav <- data.frame(rep(NA, iter))
  for (i in (1:chains)){ #for each chain: get the data from model, apply calc_var on each row
    dat <- get(outname)[[i]]
    temp <- apply(dat, 1, calc_var)
    temp<-t(temp)

    sav <- cbind(sav, temp, dat[,4]) #order: within, between, sum, ratio, e, within ...[4 times]
  }
  sav<-sav[,-1]
  rm(list=outname)
  write.table(sav, file = paste("res_", currpop, ".txt", sep = "" ), row.names=F, col.names=F)
}

```


## forest plots  
The following chunk creates forest plots of all estimates. It shows not only estimate + credible interval, but also frequency distribution, independently for the four chains. There are 5 different forest plots to make: variance among, within, their sum, ratio, and the mean timing. Each forest plot has 447 entries, which is too long for a single figure. Thus, there will be 3 columns of 50 points per page, which makes 3 pages for each of the 5 forest plots. 

```{r}
plot_forest <- function (column, xlab, xlim, ylim, num = c(1,50), scale ){ #this function will be used for all 5 forest plots
  #column = column of the dataset, 1 = variance within, 2 = between, 3 = sum, 4 = ratio, 5 = mean
  #num = rows of the dataset that should be plotted. 
  #scale = scaling factor that shrinks the histograms so they do not touch. should be smaller than 1
r <- num[2] - num[1]
plot(NA, xlim = xlim, xlab = xlab, ylim = ylim, ylab = "", bty="n", yaxt="n" )
abline(h=1:r, col="lightgrey")

h<--1 #tracks line of the forest plot
for (dataset in (num[1]:num[2])){
  h<-h+1
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))

   for ( i in 1:chains){
    dens<-density(res[,5*(i-1)+column])
    lines(x=dens$x,y = h+ scale * dens$y/max(dens$y),col=5-i)
   }
  abline(h=h,col="lightgrey")
  segments (x0 = quantile(res[,column], 0.025), x1= quantile(res[,column], 0.975), y0 = h,y1 = h,col=4) #draw 95% credible interval. this is only based on first chain
  points(x =median (res[,column]), y = h, cex=0.5, pch=22, bg=1)
  text(x=xlim[2], y = h+0.5, pos = 2, labels = unique(data$ID[data$popid == currpop]), cex = 0.8, col ="darkgrey") #author
  text(x=xlim[1], y = h+0.5, pos = 4, labels = unique(data$pop[data$popid==currpop]), cex = 0.8, col = "darkgrey") #pop
}

}



## variance within environments
l <- 50*0:9
pdf("forest_within.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 1, xlab = "Variance within", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)
#next page
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)
#next page
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

## Variance between environemnts
l <- 50*0:9
pdf("forest_between.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 2, xlab = "Variance between", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#sum of variance components
l <- 50*0:9
pdf("forest_sum.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 3, xlab = "Responsiveness", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#ratio of variance components
l <- 50*0:9
pdf("forest_ratio.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 4, xlab = "Percentage Plasticity", xlim = c(0,1),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#CDL plots
l <- 50*0:9
pdf("forest_cdl.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 5, xlab = "CDL", xlim = c(8,24),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(5, "CDL", c(8,24), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(5, "CDL", c(8,24), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(5, "CDL", c(8,24), c(1,52), c(l[9]+1,447), 0.9)
dev.off()
```


## comparison. are the results stored in res_ reliable enough?  
```{r}
estimates <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
for (dataset in 1:10){
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))
  estimates[dataset,]<-quantile(res[,3], c(0.025,0.5,0.975))
}


est2<-data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
for (dataset in 1:10){
  currpop <- unique(data$popid)[dataset]
  nam <-paste("model_", currpop, sep="")
  load(file = paste(nam, ".rds", sep=""))
  est2[dataset,] <- summary(get(nam))$quantile[4,c(1,3,5)]
}
(est2-estimates)/est2 #diff is negligible
```



## get estimates and ci
This chunk saves the estimats (within, between, sum, ration, mean) in one common table
```{r}
num<-number
estimates_w <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_b <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_s <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_r <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_e <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))

for (dataset in 1:num){
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))
  estimates_w[dataset,]<-quantile(res[,1], c(0.025,0.5,0.975))
  estimates_b[dataset,]<-quantile(res[,2], c(0.025,0.5,0.975))
  estimates_s[dataset,]<-quantile(res[,3], c(0.025,0.5,0.975))
  estimates_r[dataset,]<-quantile(res[,4], c(0.025,0.5,0.975))
  estimates_e[dataset,]<-quantile(res[,5], c(0.025,0.5,0.975))

}

estimates_w$popid <- unique(data$popid)
estimates_w$order<-sapply(estimates_w$popid,function(x){unique(data$order[data$popid == x])})
estimates_b$popid <- unique(data$popid)
estimates_b$order<-sapply(estimates_b$popid,function(x){unique(data$order[data$popid == x])})
estimates_s$popid <- unique(data$popid)
estimates_s$order<-sapply(estimates_s$popid,function(x){unique(data$order[data$popid == x])})
estimates_r$popid <- unique(data$popid)
estimates_r$order<-sapply(estimates_r$popid,function(x){unique(data$order[data$popid == x])})
estimates_e$popid <- unique(data$popid)
estimates_e$order<-sapply(estimates_e$popid,function(x){unique(data$order[data$popid == x])})

```

#forest plot, simplified
```{r}

ordered_e<-estimates_e[order(estimates_e$order),]
n<-table(ordered_e$order)
png("forest_e_simple.png",height = 480*2)
plot(NA, xlim = c(7.8,24), ylim = c(1,number),xlab = "CDL", ylab="", yaxt ="n", bty="n",xaxt="n")
axis(1, at= seq(8,24,2))
segments (x0 = ordered_e$lower, x1 = ordered_e$upper, y0=1:number,col="lightgrey")
points(x= ordered_e$med, y= 1:number, pch=22, cex=0.4 ,bg=ordered_e$order,col=ordered_e$order)
text(x=22,y = n[1]/2, labels = names(n[1]),col=1)
text(x=22,y = cumsum(n)[1:8]+n[2:9]/2, labels = names(n[2:9]),col=1)
abline(v=median(ordered_e$med),lty=2)
dev.off()

ordered_w<-estimates_w[order(estimates_w$order),]
png("forest_w_simple.png",height = 480*2)
plot(NA, xlim = c(0,0.25), ylim = c(1,number),xlab = "variance within", ylab="", yaxt ="n", bty="n",xaxt="n")
axis(1, at= seq(0,0.25,length.out = 5))
segments (x0 = ordered_w$lower, x1 = ordered_w$upper, y0=1:number,col="lightgrey")
points(x= ordered_w$med, y= 1:number, pch=22, cex=0.4 ,bg=ordered_w$order,col=ordered_w$order)
text(x=0.19,y = n[1]/2, labels = names(n[1]),col=1)
text(x=0.19,y = cumsum(n)[1:8]+n[2:9]/2, labels = names(n[2:9]),col=1)
abline(v=median(ordered_w$med),lty=2)
dev.off()



ordered_b<-estimates_b[order(estimates_b$order),]
png("forest_b_simple.png",height = 480*2)
plot(NA, xlim = c(0,0.25), ylim = c(1,number),xlab = "variance between", ylab="", yaxt ="n", bty="n",xaxt="n")
axis(1, at= seq(0,0.25,length.out = 5))
segments (x0 = ordered_b$lower, x1 = ordered_b$upper, y0=1:number,col="lightgrey")
points(x= ordered_b$med, y= 1:number, pch=22, cex=0.4 ,bg=ordered_b$order,col=ordered_b$order)
text(x=0.05,y = n[1]/2, labels = names(n[1]),col=1)
text(x=0.05,y = cumsum(n)[1:8]+n[2:9]/2, labels = names(n[2:9]),col=1)
abline(v=median(ordered_b$med),lty=2)
dev.off()

png("within_between.png")
plot(estimates_w$med~estimates_b$med,pch=22,bg=1, xlab = "variance between environments", ylab = "variance within environments",cex=0.5)
dev.off()
```





```{r funnel_plots}
a <- rep(NA, number)
  for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  a[dataset] <- nrow(data[data$popid==currpop,])
  }
png("funnel_plots.png",height = 480*1.3, pointsize = 12)
par(mfrow = c(3,1), mar = c(1,4,2,1)+0.1)
r <- estimates_e$upper-estimates_e$lower
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "",bty="n", xaxt="n", yaxt="n", cex.lab = 1.5, cex.axis=1.5) 
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-4,0,5,10,15),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r)*0.9,"A",cex=2)
r <- estimates_s$upper-estimates_s$lower
par(mar = c(1,4,1,1)+0.1)
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "Credible interval range",bty="n",xaxt="n", yaxt="n",cex.lab = 1.5, cex.axis=1.5) 
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-8,0,0.05,0.1,0.15),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r)*0.9,"B",cex=2)
r <- estimates_r$upper-estimates_r$lower
par(mar=c(4,4,1,1)+0.1)
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "Number day length treatments", ylab = "",bty="n",xaxt="n", yaxt="n" ,cex.lab = 1.5, cex.axis=1.5)
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-8,0,0.2,0.4,0.6),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r)*0.9,"C",cex=2)
dev.off()

py <- rep(NA, number)
  for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  py[dataset] <- unique(data[data$popid==currpop,3])
  }

jpy <-jitter(py)
png("publication_year.png", pointsize =12)
par(mfrow=c(3,1), mar = c(2,4,1,1)+0.1)
plot(estimates_e$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "Mean timing", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(0,10,16,22),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_e$lower, y1 = estimates_e$upper,col="darkgrey")
points(estimates_e$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)

par(mar = c(2,4,1,1)+0.1)
plot(estimates_s$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "Responsiveness", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(-1,0,0.125,0.25), labels = c(NA,0,0.125,0.25),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_s$lower, y1 = estimates_s$upper,col="darkgrey")
points(estimates_s$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)

par(mar= c(4,4,1,1)+0.1)
plot(estimates_r$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "Publication year", ylab = "Variance composition", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(-1,0,0.5,1),labels = c(NA, 0, 0.5,1 ),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_r$lower, y1 = estimates_r$upper,col="darkgrey")
points(estimates_r$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)

dev.off()

```



## combine with degN etc
```{r}
alone <- data[1,]
for (dataset in 1: number){
  currpop <- unique(data$popid)[dataset]
  alone[dataset,]<-data[data$popid==currpop,][1,]
}
alone<-alone[,-c(10,14:18)]

names(estimates_w)<-c("lower_w","med_w","upper_w","popid","order")
names(estimates_b)<-c("lower_b","med_b","upper_b","popid","order")
names(estimates_s)<-c("lower_s","med_s","upper_s","popid","order")
names(estimates_r)<-c("lower_r","med_r","upper_r","popid","order")
names(estimates_e)<-c("lower_e","med_e","upper_e","popid","order")


together <- merge(estimates_e,alone)
together <- merge(estimates_w,together, by = "popid")
together <- merge(estimates_b,together, by = "popid")
together <- merge(estimates_s,together, by = "popid")
together <- merge(estimates_r,together, by = "popid")

plot(together$med_e~together$degN)
segments(x0=together$degN,x1 = together$degN, y0 = together$lower_e, y1 = together$upper_e)
```



```{r save_data}

write.table(together, "mcmcresults.txt",append=F,sep="\t")

```
