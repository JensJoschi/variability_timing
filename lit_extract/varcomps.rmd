---
title: "read slopes"
author: "Jens Joschinski"
date: "March 15, 2018"
output: 
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
library(tidyr)
library(rjags)
library(geosphere)
```


# General description  
## Project aim  
To understand the evolvability of phenological strategies such as plasticity and bet-hedging, we correlate reaction norm properties with climate parameters. 

## Script overview  
We searched the Web of Science database for photoperiodic response curves of arthropods and selected 57 studies with individual reaction norms (dose-response curves) for 402 populations (447 reaction norms). The data was extracted with webplotdigitizer, and can be found in "lit_search/search_2018_11.xlsx".
This script does an MCMC to get estimates fo slopes, midpoints and upper and lower limit. It then calculates the variance within and between datapoints based on the same MCMC approach.It does two MCMCs with different x-axes: First, with the percentage diapause - day length reaction norm, then with a percentage diapause - day of year reaction norm. 

*Warning! The chunks take a while (+- 30 min for each MCMC chunk, +- 30 min for variance calculation, +-5 min for plots)*


# Script  
## load data  

This chunk loads the extracted raw data from the 57 studies. It requires a .csv copy of the file "lit_search/search_2018_11.xlsx". First we calculate the number of populations, sample sizes etc. Then the diapause percentages (y-axis of the reaction norm) are rounded and converted in number of successes vs number of trials. Lastly, the x-axis of the reaction norm is transformed into day of year.
```{r load_data}
data<-read.table("lit_extract/variance_comp.csv", header=T,sep=";") #pullin has been coded wrongly (1.0 instead of 100 percent); changed directly in csv script
length(unique(data$ID)) #60 studies that can be used for slope calculation
length(unique(data$spec)) #48 species
length(unique(data$genus)) #36 genera
data<-unite(data,"popid",c("ID","pop","genus","spec"),sep=".",remove=F)
(number <- length(unique(data$popid))) #458 reaction norms


#summary of #pops per region and level of detail for sample sizes

t<-data.frame(unique(data$popid))
t$reg<-NA
t$nMethod<-NA
#each unique pop has multiple rows in dataset (1 per day length)
#get this subset, take region and nMethod from first line (or use unique)
for ( i in 1:nrow(t)){ 
   t$reg[i]<-unique(data[data$popid==t[i,1],5])
   t$nMethod[i]<-unique(data[data$popid==t[i,1],9])
}
table(t$reg) #unfortunately, the information was converted to values
levels(data$region)
#Asia 16
#Europe (including caucasus): 121
#Japan 260
#US 61
#japan has 56.8% of the populations, 47.8 % of the data


table(t$nMethod)
levels(data$nmethod)

#accurate: 100
#global average:295
#pop level: 30
#NA 33


#check whether some studies have <3 pops
t<-data.frame(unique(data$ID))
t$pop<-NA
for ( i in 1:nrow(t)){ 
  t$pop[i]<-length(unique(data[data$ID==t[i,1],1]))
}
#table t$pop

data$n2[is.na(data$n2)]<-100


data$perc[data$perc<0]<-0 #sometimes percentages are estimated as e.g. -0.3% or 101%. these are rounded here to prevent errors
data$perc[data$perc>100]<-100
data$n2<-round(data$n2)
data$number<-(data$perc/100) * data$n2
data$number<-floor(data$number)
number<-length(unique(data$popid))#447
data$n<-data$number/data$n2

#check whether some studies have <4 dls
#sort (table(data$popid))
#get a vector of form "ID-population" for later labelling
nams<-unite(data,"nams",c("ID","pop"), sep = "-", remove=T)$nams 

#get additional column for "day of year". To ge that the daylength function of geosphere is reverse-used:
data$day <-NA
for (loop in 1:nrow(data)){
  dl<-daylength(data$degN[loop],173:357) #day length decline over winter at respective latitude; 357 = shortest day of year, 173 = longest
    #plot(dl) will show the day length decline from midsummer to midwinter for a given latitude. We need to find the x value which is closest to the        CDL, and record on which day it occurs
  data$day[loop]<-which.min(abs(data$dl2[loop]-dl))+173-1 #day 173 is at x == 1 ,this is why 1 is substracted
}
```



## MCMC  

Markov Chain Monte Carlo simulation. The estimates will be calculated independently for each of the 447 reaction norms. The actual model is in BUGS code, and interpreted by the R package rjags. Data is saved as .rds objects, so this chunk does not need to be repeated if changes are made to remainder of the script.
```{r jags}
#instead of saving as separate file this is put into a string, and later opened with textConnection(model_string)

model.string <- 
"model {
  
  #likelihood function
	for (i in 1:N){
		y[i] ~ dbin(success[i],trials[i])     #this incorporates sample size per point (number of individuals per day length)
		success[i] <- (d-c)/(1+exp(b*(x[i]-e)))+c #4-parameter logit curve
	}

	#priors
	b ~dunif(-100,100)       
	c ~dunif(0,1)
	d ~dunif(c,1)             #constrained to be bigger than c
	e ~dunif(min(x),max(x))   #constrained within range(day lengths)

}"


iter<-10000 #+1000 as burn-in
chains <-4

for (dataset  in (1:number)) {
  currpop <- unique(data$popid)[dataset]
  currdat<-data[data$popid==currpop,]
  
  
  model <- jags.model(textConnection(model.string), 
                    data = list(y=currdat$number, x= currdat$dl2, N = nrow(currdat),trials = currdat$n2), 
                    n.chains=chains, n.adapt=1000)

  out <- coda.samples(model,   c( 'b', 'c', 'd', 'e'), iter)
  outname <- paste("model_", currpop, sep="")
  assign(outname, out)
  save(list = outname ,file = paste(outname, ".rds", sep = ""))
  rm(list= outname)
}

```

## MCMC2  
exact copy of above, only that x = dl2 is replaced by x = day - and outname is changed
```{r jags}
for (dataset  in (1:number)) {
  currpop <- unique(data$popid)[dataset]
  currdat<-data[data$popid==currpop,]
  
  
  model <- jags.model(textConnection(model.string), 
                    data = list(y=currdat$number, x= currdat$day, N = nrow(currdat),trials = currdat$n2), 
                    n.chains=chains, n.adapt=1000)

  out <- coda.samples(model,   c( 'b', 'c', 'd', 'e'), iter)
  outname <- paste("model_day_", currpop, sep="")
  assign(outname, out)
  save(list = outname ,file = paste(outname, ".rds", sep = ""))
  rm(list= outname)
}

```

### quality control  
```{r}
for (i in 1:447){
  t <-data[data$popid==unique(data$popid)[i],]
  png(file = paste(i, ".png", sep=""))
  #par(mfrow= c(1,2))
  plot(t$perc~t$day,type="b",main = t$popid[1],ylim=c(0,100))
 # plot(t$perc~t$dl2,type="b")
  dev.off()}

#after sorting out by hand 22 reaction norms were deemed unidentifiable:
sort_out<-unique(data$popid)[c(1,25,26,27,28,36,43,47,86,169,170,171,172,200,209,220,289,326,367,368,369,443)]
```

## calculation of variance components  
We follow the trace of the MCMC (which was saved in previous step), and calculate the variance components at each iteration. The frequency distribution of variance estimates is used to get estimate  + credible interval.
```{r calc_variance}

logcurve<-function(x,b,c,d,e){(d-c)/(1+exp(b*(x-e)))+c} #4-paramter dose-response curve, eq.1

calc_var <- function(row){
  b<-row[1]
  c<-row[2]
  d<-row[3]
  e<-row[4]
  
  x<-seq(e-100,e+100, length.out =1000)
  y<-(d-c)/(1+exp(b*(x-e)))+c #produces logitcurve along a segment of mean +-100 days
  w <- sum(y*(1-y))/1000 #variance within, eq.2
  b <- sd(y)^2  #variance among, eq.3
  c(w, b,  b+w, b/(b+w)) #returns variance within, variance among, their sum (eq.5), and the ratio (eq.4)
}


for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  outname <- paste("model_day_", currpop, sep="")
  load(paste(outname, ".rds", sep=""))
  sav <- data.frame(rep(NA, iter))
  if (currpop %in% sort_out){sav[,2:21]<-NA}else{ #makes sure that day length reaction norm is ignored; but cdl is saved anyway
    for (i in (1:chains)){ #for each chain: get the data from model, apply calc_var on each row
     dat <- get(outname)[[i]]
     temp <- apply(dat, 1, calc_var)
     temp<-t(temp)

      sav <- cbind(sav, temp, dat[,4]) #order: within, between, sum, ratio, e, within ...[4 times]
    }
  }
  sav<-sav[,-1] #the NA column
  rm(list=outname)
  outname <- paste("model_", currpop, sep="")
  load(paste(outname, ".rds", sep=""))
  for (i in (1:chains)){
    sav[,chains*5 + i]<-get(outname)[[i]][,4]
  }
  rm(list=outname)
  write.table(sav, file = paste("res_", currpop, ".txt", sep = "" ), row.names=F, col.names=F)
}

```

## forest plots  
The following chunk creates forest plots of all estimates. It shows not only estimate + credible interval, but also frequency distribution, independently for the four chains. There are 6 different forest plots to make: variance among, within, their sum, ratio, and the mean timing, as well as critical day length. Each forest plot has 447 entries, which is too long for a single figure. Thus, there will be 3 columns of 50 points per page, which makes 3 pages for each of the 6 forest plots. 

```{r}
plot_forest <- function (column, xlab, xlim, ylim, num = c(1,50), scale ){ #this function will be used for the 5 forest plots taht were converted into mean diapause
  #column = column of the dataset, 1 = variance within, 2 = between, 3 = sum, 4 = ratio, 5 = mean
  #num = rows of the dataset that should be plotted. 
  #scale = scaling factor that shrinks the histograms so they do not touch. should be smaller than 1
r <- num[2] - num[1]
plot(NA, xlim = xlim, xlab = xlab, ylim = ylim, ylab = "", bty="n", yaxt="n" )
abline(h=1:r, col="lightgrey")

h<--1 #tracks line of the forest plot
for (dataset in (num[1]:num[2])){
  h<-h+1
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))
  if (!is.na(res[,1])){ #should leave 22 empy lines
   for ( i in 1:chains){
    dens<-density(res[,5*(i-1)+column])
    lines(x=dens$x,y = h+ scale * dens$y/max(dens$y),col=5-i)
   }
  abline(h=h,col="lightgrey")
  segments (x0 = quantile(res[,column], 0.025), x1= quantile(res[,column], 0.975), y0 = h,y1 = h,col=4) #draw 95% credible interval. this is only based on first chain
  points(x =median (res[,column]), y = h, cex=0.5, pch=22, bg=1)
  }
  text(x=xlim[2], y = h+0.5, pos = 2, labels = unique(data$ID[data$popid == currpop]), cex = 0.8, col ="darkgrey") #author
  text(x=xlim[1], y = h+0.5, pos = 4, labels = unique(data$pop[data$popid==currpop]), cex = 0.8, col = "darkgrey") #pop
}

}



## variance within environments
l <- 50*0:9
pdf("forest_within.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 1, xlab = "Variance within", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)
#next page
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)
#next page
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(1, "Variance within",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(1, "Variance within", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

## Variance between environemnts
l <- 50*0:9
pdf("forest_between.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 2, xlab = "Variance between", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(2, "Variance between",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(2, "Variance between", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#sum of variance components
l <- 50*0:9
pdf("forest_sum.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 3, xlab = "Responsiveness", xlim = c(0,0.25),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(3, "Responsiveness",c(0,0.25), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(3, "Responsiveness", c(0,0.25), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#ratio of variance components
l <- 50*0:9
pdf("forest_ratio.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 4, xlab = "Percentage Plasticity", xlim = c(0,1),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(4, "Percentage Plasticity",c(0,1), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(4, "Percentage Plasticity", c(0,1), c(1,52), c(l[9]+1,447), 0.9)
dev.off()

#mean timing plots
l <- 50*0:9
pdf("forest_mean_timing.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_forest(column = 5, xlab = "Mean Timing", xlim = c(120,355),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_forest(5, "Mean Timing", c(120,355), c(1,52), c(l[9]+1,447), 0.9)
dev.off()


#the cdl plot is slightly different, it can be done on all 447 reation norms (22 become NA after conversion to mean diapause, but that does not matter for cdl). Thus there is a slightly different function for this one
plot_cdl <- function (xlab, xlim, ylim, num = c(1,50), scale ){ #this function will be used for all 5 forest plots
  #column = column of the dataset, 1 = variance within, 2 = between, 3 = sum, 4 = ratio, 5 = mean
  #num = rows of the dataset that should be plotted. 
  #scale = scaling factor that shrinks the histograms so they do not touch. should be smaller than 1
r <- num[2] - num[1]
plot(NA, xlim = xlim, xlab = xlab, ylim = ylim, ylab = "", bty="n", yaxt="n" )
abline(h=1:r, col="lightgrey")

h<--1 #tracks line of the forest plot
for (dataset in (num[1]:num[2])){
  h<-h+1
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))

   for ( i in 1:chains){
    dens<-density(res[,20+i])
    lines(x=dens$x,y = h+ scale * dens$y/max(dens$y),col=5-i)
   }
  abline(h=h,col="lightgrey")
  segments (x0 = quantile(res[,21], 0.025), x1= quantile(res[,21], 0.975), y0 = h,y1 = h,col=4) #draw 95% credible interval. this is only based on first chain
  points(x =median (res[,21]), y = h, cex=0.5, pch=22, bg=1)
  
  text(x=xlim[2], y = h+0.5, pos = 2, labels = unique(data$ID[data$popid == currpop]), cex = 0.8, col ="darkgrey") #author
  text(x=xlim[1], y = h+0.5, pos = 4, labels = unique(data$pop[data$popid==currpop]), cex = 0.8, col = "darkgrey") #pop
}

}
l <- 50*0:9
pdf("forest_cdl.pdf",width=7, height = 7*2, pointsize=12)
par(mfrow = c(1,3),mar = c(4,2,1,0))
plot_cdl(xlab = "Critical day length", xlim = c(8,24),ylim = c(1,52), num = c(l[1]+1,l[2]),scale=  0.9)
plot_cdl("Critical day length", c(8,24), c(1,52), c(l[2]+1,l[3]), 0.9)
plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[3]+1,l[4]), 0.9)

plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[4]+1,l[5]), 0.9)
plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[5]+1,l[6]), 0.9)
plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[6]+1,l[7]), 0.9)

plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[7]+1,l[8]), 0.9)
plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[8]+1,l[9]), 0.9)
plot_cdl( "Critical day length", c(8,24), c(1,52), c(l[9]+1,447), 0.9)
dev.off()
```




## get estimates and ci
This chunk saves the estimats (within, between, sum, ration, mean) in one common table
```{r}
num<-number
estimates_w <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_b <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_s <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_r <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_e <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))
estimates_cdl <- data.frame(lower = rep(NA,num), med = rep(NA,num), upper = rep(NA,num))

for (dataset in 1:num){
  currpop <- unique(data$popid)[dataset]
  res<-read.table(file = paste("res_", currpop, ".txt", sep = "" ))
   if (!(currpop %in% sort_out)){
  estimates_w[dataset,]<-quantile(res[,1], c(0.025,0.5,0.975))
  estimates_b[dataset,]<-quantile(res[,2], c(0.025,0.5,0.975))
  estimates_s[dataset,]<-quantile(res[,3], c(0.025,0.5,0.975))
  estimates_r[dataset,]<-quantile(res[,4], c(0.025,0.5,0.975))
  estimates_e[dataset,]<-quantile(res[,5], c(0.025,0.5,0.975))
   }
  estimates_cdl[dataset,]<-quantile(res[,21], c(0.025,0.5,0.975)) #this will be read even if all other data is NA. cdl is done before conversion to julian days and not affected by sort_out procedure

}

estimates_w$popid <- unique(data$popid)
estimates_w$order<-sapply(estimates_w$popid,function(x){unique(data$order[data$popid == x])})
estimates_b$popid <- unique(data$popid)
estimates_b$order<-sapply(estimates_b$popid,function(x){unique(data$order[data$popid == x])})
estimates_s$popid <- unique(data$popid)
estimates_s$order<-sapply(estimates_s$popid,function(x){unique(data$order[data$popid == x])})
estimates_r$popid <- unique(data$popid)
estimates_r$order<-sapply(estimates_r$popid,function(x){unique(data$order[data$popid == x])})
estimates_e$popid <- unique(data$popid)
estimates_e$order<-sapply(estimates_e$popid,function(x){unique(data$order[data$popid == x])})
estimates_cdl$popid <- unique(data$popid)
estimates_cdl$order<-sapply(estimates_e$popid,function(x){unique(data$order[data$popid == x])})

```


```{r funnel_plots}
a <- rep(NA, number)
  for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  a[dataset] <- nrow(data[data$popid==currpop,])
  }
png("funnel_plots.png",height = 480*1.3, pointsize = 12)
par(mfrow = c(4,1), mar = c(1,4,1,1)+0.1, oma = c(3,3,0,0))
r <- estimates_e$upper-estimates_e$lower
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "",bty="n", xaxt="n", yaxt="n", cex.lab = 1.5, cex.axis=1.5) 
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-40,0,50,100,150,200),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r,na.rm=T)*0.9,"A",cex=2)
r <- estimates_cdl$upper-estimates_cdl$lower
par(mar = c(1,4,1,1)+0.1)
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "",bty="n",xaxt="n", yaxt="n" ,cex.lab = 1.5, cex.axis=1.5)
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-8,0,5,10,15),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r,na.rm=T)*0.9,"B",cex=2)
r <- estimates_s$upper-estimates_s$lower
par(mar = c(1,4,1,1)+0.1)
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "",bty="n",xaxt="n", yaxt="n",cex.lab = 1.5, cex.axis=1.5,xpd=T) 
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-8,0,0.05,0.1,0.15),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r,na.rm=T)*0.9,"C",cex=2)
r <- estimates_r$upper-estimates_r$lower
par(mar=c(1,4,1,1)+0.1)
plot(r~jitter(a),pch=21, cex=1, bg = 1, col=1, xlab = "", ylab = "",bty="n",xaxt="n", yaxt="n" ,cex.lab = 1.5, cex.axis=1.5)
axis(1, at = c(-3,5,10,15,20,25),cex.lab = 1.5, cex.axis=1.5)
axis(2, at = c(-8,0,0.2,0.4,0.6),cex.lab = 1.5, cex.axis=1.5)
text(4,max(r,na.rm=T)*0.9,"D",cex=2)
mtext ("Number day length treatments",side = 1, line = 2, outer =T, cex=1.5)
mtext ("Credible interval range", side = 2, line = 0, outer =T,cex=1.5)

dev.off()

py <- rep(NA, number)
  for (dataset in 1:number){
  currpop <- unique(data$popid)[dataset]
  py[dataset] <- unique(data[data$popid==currpop,3])
  }

jpy <-jitter(py)
png("publication_year.png", pointsize =12)
par(mfrow=c(4,1), mar = c(2,6,1,1)+0.1, oma = c(3,2,0,0))
plot(estimates_e$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "Mean timing", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(180,220,260,300,340),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_e$lower, y1 = estimates_e$upper,col="darkgrey")
points(estimates_e$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)

plot(estimates_cdl$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "CDL", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(-1,9,12,15,18,21), labels = c(NA,9,12,15,18,21),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_cdl$lower, y1 = estimates_cdl$upper,col="darkgrey")
points(estimates_cdl$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)



plot(estimates_s$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "Responsiveness", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(-1,0,0.125,0.25), labels = c(NA,0,0.125,0.25),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_s$lower, y1 = estimates_s$upper,col="darkgrey")
points(estimates_s$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)


plot(estimates_r$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1, xlab = "", ylab = "Variance \ncomposition", bty= "n", cex.lab = 1.5, cex.axis = 1.5, xaxt = "n", yaxt = "n")
axis(1, at = c(1970,1980,1990,2000,2010,2020),cex.lab = 1.5, cex.axis = 1.5)
axis(2, at = c(-1,0,0.5,1),labels = c(NA, 0, 0.5,1 ),cex.lab = 1.5, cex.axis = 1.5)
segments(x0 = jpy, y0 =estimates_r$lower, y1 = estimates_r$upper,col="darkgrey")
points(estimates_r$med ~ jpy, pch= 21, cex= 1, bg= 1, col= 1)
mtext ("Publication year", side = 1, line = 1, outer =T,cex=1.5)
dev.off()

```



## combine with degN etc
```{r}
alone <- data[1,]
for (dataset in 1: number){
  currpop <- unique(data$popid)[dataset]
  alone[dataset,]<-data[data$popid==currpop,][1,]
}
alone<-alone[,-c(10,14:18)]

names(estimates_w)<-c("lower_w","med_w","upper_w","popid","order")
names(estimates_b)<-c("lower_b","med_b","upper_b","popid","order")
names(estimates_s)<-c("lower_s","med_s","upper_s","popid","order")
names(estimates_r)<-c("lower_r","med_r","upper_r","popid","order")
names(estimates_e)<-c("lower_e","med_e","upper_e","popid","order")
names(estimates_cdl)<-c("lower_cdl","med_cdl","upper_cdl","popid","order")

together <- merge(estimates_e,alone)
together <- merge(estimates_w,together, by = "popid")
together <- merge(estimates_b,together, by = "popid")
together <- merge(estimates_s,together, by = "popid")
together <- merge(estimates_r,together, by = "popid")
together <- merge(estimates_cdl,together, by = "popid")
plot(together$med_e~together$degN)
segments(x0=together$degN,x1 = together$degN, y0 = together$lower_e, y1 = together$upper_e)
```



```{r save_data}
together<-together[,-c(9,13,17,21,22)]
write.table(together, "mcmcresults.txt",append=F,sep="\t")

```
