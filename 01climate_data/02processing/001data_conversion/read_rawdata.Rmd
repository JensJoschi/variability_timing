---
title: "clim_var_coldsum"
author: "Jens Joschinski"
date: "February 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#setwd(dirname(dirname(dirname(getwd()))))
```
# General description  
## Aim  
The aim of this project is to correlate climate variability with variability in seasonal timing. Is the slope in seasonal responses a bet-hedging trait, i.e., is it adaptive to spread one's timing in more variable conditions?  


### Overview  
This script prepares climate station data to calculate winter variability. The actual calculation of winter variability is in separate R scripts in subfolders of the folder "01climate_data/02processing".  

This scripts first connects with the NOAA server to get a list of available climate stations together with their latitudes and longitudes. This is the only time a connection to the server is required.  

The script then open a pre-processed (with PERL) dataset, which includes daily average temeratures for all months, years and stations all over the world (~3million). The data is merged with the climate station data, cleaned, and saved as workspace to be used by other scripts.  

### Specific description  

The data was generated with R version `r getRversion()`. It uses the GHCN-daily dataset by NOAA:
```{r, echo =F}
library(textreadr)
text<- read_document("01raw/ghcnd_all/ghcnd-version.txt")
print (text[1])
```


### Script  

#### Read locations data  

``` {r, include = F}
library(RCurl)
library(readr)
library(data.table)
```

```{r}
url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
#this dataset is fixed-width delimited, requiring a few additional steps
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional seps
  widths=c(11, 9, 10, 7,2,35)
)
#plot(locations[,2]~locations[,3],xlab = "longitude",ylab="latitude",pch=21, cex=0.1)
```


This dataset has only station ID, coordinates, altitude and name of the station, no actual data. The data was pre-processed with a PERL script, which now needs to be merged with the location data.


#### Read and merge climate station data  
```{r}
converted<-fread(paste(getwd(),"/02processing/001data_conversion/converted.txt",sep=""), sep ="|", na.strings=c("NA","-9999"),verbose=F)
#this dataset has been made with the perl script conversion.pl in subfolder 01climatedata/02processing/001data_conversion
```


*this perl script takes the NOAA GHCN daily dataset (daily weather data for ~50 ys, around the whole world), and extracts
daily average temperatures. it ignores all other data flags that are inside that dataset. The format of the old datasets is fixed-width delimited, the new one will be delimited by "|".
the new data is sorted in the following way:  
station1 | year 1 | month 1 | T(day1) | T(day2) |...| T(day31)  
station1 | year 1 | month 2...  
station1 | year 2 | month 1...  
station2| year 1 | month 1...  
missing data, and non-existent days (eg Feb 31) are called NA.  
this script uses a local copy of the dataset from "ftp.ncdc.noaa.gov/pub/data/ghcn/daily/"; the .tar.gz file has to be extracted first (separate folder with 100,000 text files)*


Merging of the dataset with location data:


```{r}
newset<-merge.data.frame(converted,locations,by=1)
rm(converted)
#now newset contains daily temps of all stations + their coordinates and names

names(newset)<-c("ID","year","month",1:31,"lat","lon","alt","name","?")
#factors to numeric etc, further cleanup
newset<-droplevels(newset)
newset[,2]<-(as.integer(as.character(newset[,2])))
newset[,3]<-(as.integer(as.character(newset[,3])))
str(newset)
newset$alt[newset$alt==-999.9]<-NA
nona<-newset
nona$alt<-nona$alt+350
nona<-nona[is.na(nona$alt)==F,]
plot(nona$lat~nona$lon,col=rgb(nona$alt,0,0,maxColorValue = max(nona$alt)),main="location of climate stations, altitude in red",cex=0.2)
rm(nona)
```

#### saving output  

The raw data was loaded successfully, and information about coordinates and altitude was added. Now the dataset will be saved as workspace, so that it can be used by other scripts.


```{r}
rm(locations)
rm(url)
save.image(paste(getwd(),"/02processing/001data_conversion/Rworkspace.RData",sep=""))
```

