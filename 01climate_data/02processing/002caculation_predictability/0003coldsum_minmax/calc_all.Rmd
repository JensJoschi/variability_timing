---
title: "Calculation of winter variability: X days below Y °C"
author: "Jens Joschinski"
date: "September 17, 2018"
output: 
    md_document:
        variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
library(RCurl)
library(readr)
library(data.table)
library(textreadr)
library(tidyr)
library(dplyr)
library(stringr)
library(magrittr)
library(imputeTS)
#threshold<-50
#t2<-10
```

# General description  
## Aim  
The aim of this project is to correlate climate variability with variability in seasonal timing. Is the slope in seasonal responses a bet-hedging trait, i.e., is it adaptive to spread one's timing in more variable conditions?  


### Overview  
The script calculates mean winter onset, winter variability and winter predictability for all climate stations around the world. The data will be combined with reaction norms of various organisms in separate scripts. 
The script first has a general part: it uses the daily minimum and maximum temeratures for all months, years and climate stations all over the world from the GHCN dataste (~12million X 31 days).These were pre-processed with PERL and saved as two txt files. The two datasets are now opened, cleaned, and the mean of min and max is calculated for each day. The resulting workspace with one matrix of 11milx31days is saved.  

The remaining parts of the script individually calculate mean winter, winter variability and winter predictability (based on two different ways)

Further processing uses smaller chunks of data at a time (30 chunks of 1k climate stations). These are subsequently read in and transformed to differen format (so that 1 row has temperature of a whole year). Then I calculate winter onset as xth day that temperature falls below y°C. For each station I calculate mean winter onset (weighted by available data per year), standard deviation in winter onset, and winter predictability: I make a linear regression through the last 31 days before winter onset of each year, and calculate the slope. The inverse of the standard deviation of this slope estimate across years is winter predictability.Finally, all data of one station is put into a single vector, and I apply a non-linear least square regression: The data is expected to follow a sine-curve with a period of 365 days, and I record for each station amplitude (difference between summer and winter), phase angle (day of peak summer) and intercept (mean temperature). The results (mean winter onset, sd, correlation, nls-parameters) are saved in a text file. All of this is done on all chunks of data, and the results are appended in a single file.



## Specifics

The data was generated with R version `r getRversion()`. It uses the GHCN-daily dataset by NOAA:
```{r, echo =F}
text<- read_document("01climate_data/01raw/ghcnd_all/ghcnd-version.txt")
print (text[1])
```


## Script  

### calculate mean temperature  

The following chunk opens the two datasets that were extracted from the GHCN-D database (min and max temperature). Those rows that occur in both datasets are used to calculte mean temperature. Everything else is discarded 

```{r join}
converted<-fread("01climate_data/02processing/001data_conversion/converted_max.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F) #12,616,909 rows
length(unique(converted$V1)) #34,145                                                                  
converted2<-fread("01climate_data/02processing/001data_conversion/converted_min.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F) #12,658,376
length(unique(converted2$V1)) #34,052 


converted<-unite_(converted,"new_id",c("V1","V2","V3"),remove=FALSE)
converted2<-unite_(converted2,"new_id",c("V1","V2","V3"),remove=FALSE)

names(converted)<-c("new_id","ID","year","month",1:31)
names(converted2)<-c("new_id","ID","year","month",1:31)

n1<-unique(converted$new_id)
n2<-unique(converted2$new_id)
both <- n1[n1 %in% n2]
rm(n1)
rm(n2)

converted2<-converted2[converted2$new_id %in% both]
converted<-converted[converted$new_id %in% both]
nrow(converted)
rm(both)
#we are now down to 12.5 million rows in each dataset

converted<-arrange(converted, new_id)
converted2<-arrange(converted2, new_id)

join<-converted
join[,5:35]<-(converted[,5:35]+converted2[,5:35])/2
plot(join[500:550,10],col=2,pch=21,bg=2)
segments(1:51,converted[500:550,10],1:51,converted2[500:550,10])
#seems to work
rm(converted)
rm(converted2)


```


#### clean data  
The following chunk removes 
- data from southern hemisphere
- years with >50% data present
- stations with <3 years of data
```{r clean_data}
#remove all stations from southern hemisphere

#url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
url<-"ghcnd-stations.txt" #use local copy instead of url
#this dataset is fixed-width delimited, requiring a few additional steps
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional separators
  widths=c(11, 9, 10, 7,2,35)
)

badlist <- locations[locations[,2]<0,1]  #latitudes smaller 0°N
rm(locations)
rm(url)

join<-join[!(join$ID %in% badlist),]
nrow(join) #11,755,351
rm(badlist)


#remove all years with >50% missing
join$days<-rowSums(!is.na(join[,5:ncol(join)]))
x<-group_by(join,ID,year)
y<-tally(x,days)
nrow(y) #1,058,817station-years
#nrow(y[y$n>364,])/nrow(y)#42.8% off all station-years are complete (no NA)
#nrow(y[y$n>350,])/nrow(y)#66.3% have more than 350 days
#nrow(y[y$n>300,])/nrow(y)#78.7% have more than 300 days
nrow(y[y$n<180,])/nrow(y)#10.4% are less than half complete
inlist<-y$ID[y$n>182]
join<-join[join$ID %in% inlist,]
nrow(join)# 11,675,760
rm(inlist)

#remove all stations with <3 ys of data
x<-count(join,ID,year)
x$ID<-factor(x$ID)
ny<-count(x,ID)
mean(ny$nn)*n_distinct(x$ID) #mean no years * total number stations = number rows in dataset
include<-ny$ID[ny$nn>2] 
length(include) #28,346 stations can be included
length(ny$nn) #29,256 in total

rm(x)
rm(y)
rm(ny)
join<-join[join$ID %in% include,]
rm(include)
nrow(join)#11,648,376
length(unique(join$ID)) #28,346
```
#### saving  

a bit of cleaning, and saving the workspace
```{r save}
join<-join[,-1]
names(join)<-c("ID","year","month",1:31,"days")
mymonths <- c("07Jan","08Feb","09Mar",
              "10Apr","11May","12Jun",
              "01Jul","02Aug","03Sep",
              "04Oct","05Nov","06Dec")
join$month<-mymonths[join$month]  #when it is sorted alphabetically it now starts in july, and ends in jun
rm(mymonths)
converted<-join
rm(join)
save.image(file = "converted.RData") 
#row 1         ID year month   1     2     3   4     5     6   7   8   9    10  11  12  13  14  15
#1 ACW00011604 1949 07Jan 253 258.5 252.5 258 255.5 255.5 253 242 247 230.5 228 236 214 225 228
#     16  17    18  19    20    21    22  23  24    25  26    27    28  29  30    31
#1 244.5 239 241.5 233 244.5 247.5 244.5 239 239 233.5 250 244.5 241.5 247 239 244.5
```



#### transposing data
the Data so far has an awkward structure, with one month per row. The following chunk puts all data from one year to one line, and also starts the "year" in july.

```{r transform_matrix}
converted<-converted[,-35]
x<-unique(converted$ID)
for(i in 1:length(x)){
station<-converted[converted$ID==x[i],]
station<-
  station %>% 
  gather(key="day",value="temperature",-(1:3)) %>% #makes one very long dataframe with 2 columns
  mutate (day=str_pad(day,2,pad="0")) %>%  #this makes leading zeros where they are needed
  mutate (temperature=as.numeric(temperature)) %>%
  arrange (ID,year,month,day) %>%
  unite_("un",c("month","day")) %>%  #makes new col with new ID
  spread(key="un",value="temperature")#puts in new wide format with 365 columns (month-day)

#length(station$`08Feb_29`) #not many, due to leapyears
x<-NA
for(i in 1:ncol(station)){x[i]<-length(unique(station[,i]))}
station<-station[-which(x==1)]  #dates like 30 feb are erased
write_delim(station,"temp.txt",append=T)
}
```


### 1. calculate mean winter onset
```{r mean}   

x$navector<-navector
summarise(x,navector)
for (loop in 1:length(x)){
  station<-converted[converted$ID %in% x$ID,]
  
}

for (loop in 1:29){ #this loop goes through the 29 chunks of 1k stations (last chunk has only 963 entries)
station<-converted[converted$ID %in% x[((loop-1)*1000+1):(1000*loop)],]
station<-turn_around(station)

#remove years with <182 days of data
n<-rowSums(!is.na(station))
station<-station[n>184,] #2 more becasue they always have id and year

#remove stations with less than 3 years
temp<-table(station$ID)
temp <-as.data.frame(t(temp)) 
temp<-temp[temp$Freq<3,2]
temp<-droplevels(temp)
station<-station[!(station$ID %in% temp),]
station<-droplevels(station)

#vectors to store results
mean_winter<-rep(NA,length(unique(station$ID)))
sd_winter<- rep(NA,length(unique(station$ID)))
nyears<-rep(NA,length(unique(station$ID)))


for (i in 1:length(unique(station$ID))){ #this loop goes through each of the 1 k stations
  
  ####read in station####
  id<-unique(station$ID)[i]
  s<-filter(station,ID==id)
  ndays<-rowSums(!is.na(s))-2
  #format of the data: year 1, day 1...365
  #                    year 2, day 1...365
  
   
  
  ####calculate winter arrival#####
 # make a binomial matrix with all Temps<threshold = 1, all others =0(including NA)

  bino<-rep(0,nrow(s)*(ncol(s)-2))
  bino[s[,3:368]<threshold]<-1
  bino<-matrix(data=bino,nrow=nrow(s),ncol=ncol(s)-2)

  #exclude years that never reach winter (less than x days below y°C)
  used<-rowSums(bino)>t2
  
  #exclude stations that reach winter in less than 3 years (this is different from having less than 3 ys of data)
  if (sum(used) <3) {
    mean_winter[i]<-NA
    sd_winter[i]<-NA
     next
  }
  
  bino<-bino[used,]
  s<-s[used,]
  w_on<-rep(NA,nrow(bino))
  ndays<-ndays[used]
  
  #calcualte row-wise cumulative sums
  #function cumsum works only on columns, so matrix needs transposing
  bino <- 
    bino %>%
    t %>% 
    as.data.frame %>%
    cumsum %>%
    t %>%
    as.data.frame
  
  bino<-bino==t2 #turns it into TRUE/FALSE
 w_on<- max.col(bino, ties.method="first")#first "true" ="threshold reached" in each year
 nyears[i] <- length(w_on)
  
  ##calculate mean and sd in winter arrival (both weighted)####
 
  mean_winter[i]<-weighted.mean(w_on,ndays)
  weights<-ndays/366
  upper <- sum(weights*(w_on-mean_winter[i])^2)
  lower <- sum(weights)-1
  sd_winter[i] <- sqrt(upper/lower)
  
  #clean up memory####
  rm(upper)
  rm(lower)
  rm(weights)
  rm(ndays)
  rm(bino)

  } #next station


#####gather and save results####
reslist<-data.frame(unique(station$ID),mean_winter,sd_winter,nyears,p,A,phi,int,beta_y)

names(reslist)<-c("ID","meanwinter","sd winter","nyears","predictability","Amplitude","phase_angle","intercept","beta" )
write.table(reslist,"results.txt",append=TRUE,quote=FALSE,row.names=FALSE,col.names=FALSE)
} #next chunk
```


#### data structure  
Due to problems with memory allocation, the big (26 million rows) dataset needs to be processed piece-wise. I chose 30 chunks of 1000 climate stations each. 
The following steps have to be done on that 1000 station dataset

* change data structure from 
    station 1 year 1 month 1 day 1,2,3..31
    station 1 year 1 month 2 day 1,2,3..31  
  to:  
    station 1 year 1 day 1,2,3.. 365
    station 1 year 2 day 1,2,3.. 365  
    
* make subset with 1 station and
  * calculate winter onset in each year  
  * calculate mean winter onset (weighted by number of days per year)  
  * calculate weighted standard deviation in winter onset  
  * calculate standard deviation in slopes of temperature decline before winter  
  * change structure to station 1 day 1....20,000 and make nls regression  





### actual calculation  
Do the calculations, piecewise for 29 chunks of data; save the results
```{r}   

x<-unique(converted$ID)
for (loop in 1:29){ #this loop goes through the 29 chunks of 1k stations (last chunk has only 963 entries)
station<-converted[converted$ID %in% x[((loop-1)*1000+1):(1000*loop)],]
station<-turn_around(station)

#remove years with <182 days of data
n<-rowSums(!is.na(station))
station<-station[n>184,] #2 more becasue they always have id and year

#remove stations with less than 3 years
temp<-table(station$ID)
temp <-as.data.frame(t(temp)) 
temp<-temp[temp$Freq<3,2]
temp<-droplevels(temp)
station<-station[!(station$ID %in% temp),]
station<-droplevels(station)

#vectors to store results
mean_winter<-rep(NA,length(unique(station$ID)))
sd_winter<- rep(NA,length(unique(station$ID)))
nyears<-rep(NA,length(unique(station$ID)))
p<-rep(NA,length(unique(station$ID)))
beta_y<-rep(NA,length(unique(station$ID)))
#beta_m<-rep(NA,length(unique(station$ID)))
A<-rep(NA,length(unique(station$ID)))
phi<-rep(NA,length(unique(station$ID)))
int<-rep(NA,length(unique(station$ID)))


for (i in 1:length(unique(station$ID))){ #this loop goes through each of the 1 k stations
  
  ####read in station####
  id<-unique(station$ID)[i]
  s<-filter(station,ID==id)
  ndays<-rowSums(!is.na(s))-2
  #format of the data: year 1, day 1...365
  #                    year 2, day 1...365
  
   
  
  ####calculate winter arrival#####
 # make a binomial matrix with all Temps<threshold = 1, all others =0(including NA)

  bino<-rep(0,nrow(s)*(ncol(s)-2))
  bino[s[,3:368]<threshold]<-1
  bino<-matrix(data=bino,nrow=nrow(s),ncol=ncol(s)-2)

  #exclude years that never reach winter (less than x days below y°C)
  used<-rowSums(bino)>t2
  
  #exclude stations that reach winter in less than 3 years (this is different from having less than 3 ys of data)
  if (sum(used) <3) {
    mean_winter[i]<-NA
    sd_winter[i]<-NA
     next
  }
  
  bino<-bino[used,]
  s<-s[used,]
  w_on<-rep(NA,nrow(bino))
  ndays<-ndays[used]
  
  #calcualte row-wise cumulative sums
  #function cumsum works only on columns, so matrix needs transposing
  bino <- 
    bino %>%
    t %>% 
    as.data.frame %>%
    cumsum %>%
    t %>%
    as.data.frame
  
  bino<-bino==t2 #turns it into TRUE/FALSE
 w_on<- max.col(bino, ties.method="first")#first "true" ="threshold reached" in each year
 nyears[i] <- length(w_on)
  
  ##calculate mean and sd in winter arrival (both weighted)####
 
  mean_winter[i]<-weighted.mean(w_on,ndays)
  weights<-ndays/366
  upper <- sum(weights*(w_on-mean_winter[i])^2)
  lower <- sum(weights)-1
  sd_winter[i] <- sqrt(upper/lower)
  
  #clean up memory####
  rm(upper)
  rm(lower)
  rm(weights)
  rm(ndays)
  rm(bino)


  ##calculate standard deviation in slopes of last 30 days before winter onset####
  #idea: transpose and use gather to make 1 long vector ("day1:day 10,000" only that they are not called like that)
        #cut at (winter onset - 30) and winter onset of each year (multiple pieces)
        #clean incomplete data
        #make correlations of temperature ~ time to get the slope of each year
        #calculate sd of the slopes between years
  
  #transpose
  rownames(s)<-1:nrow(s) #these are years 1:x
  n<-gather(as.data.frame(t(s[3:368]))) #turns into long format with 2 columns:
  #key = 1,1,1...(366 times), 2,2,2...(366 times),..,40,40... (assuming there are 40 years)
  #value = temperature year 1, day 1:366, year 2, day 1.... 
  n$xvec<-rep(1:(ncol(s)-2),length(unique(n$key))) #xvec is days after 1st july, so it repeats 1:366, 40 times
  n$key<-as.integer(n$key)
  
  #cut
  n<-filter(n,xvec<w_on[key] & xvec >= w_on[key]-31)  #now dataset contains only last 31 days before winter onset in that respective year
  
  #clean
  n2<-group_by(n,key)
  sm<-summarise(n2,sum(!is.na(value))>10) #makes a tibble with 2 cols, key and usable==T or F
  usable<-pull(sm[sm[,2]==T,1]) #makes a vector with all years that have >10 days
  n<-n[n$key %in% usable,]

  #slope estimates
  slopes<-rep(NA,length(usable))
  for (keygroup in usable){ 
    y<-filter(n,key == keygroup)
    slopes[keygroup]<-coef(lm(y$value~y$xvec))[2]
  }
  #translation: for each year take data of that year, save slope estimate from lm
  p[i]<-sd(slopes,na.rm=T)


  
  #make non-linear least-square regression ####

# The function tries up to three times per climate station, with different starting parameters.
 n<-gather(as.data.frame(t(s[3:ncol(s)])))

 parms<-wrap_error(n$value,s_A=400,s_phi=pi/2,s_c=200)
  if (parms[1]==FALSE){parms<-wrap_error(n$value,s_A=40,s_phi=pi,s_c=200)}
  if (parms[1]==FALSE){parms<-wrap_error(n$value,s_A=40,s_phi=2*pi,s_c=200)}
  if (parms[1]==FALSE){print (paste("failed in ",i));parms<-c(-9999,-9999,-9999)}

 A[i]<-parms[1]
 phi[i]<-parms[2]
 int[i]<-parms[3]
 
  
 ####  spectral density estimate: beta ###
#two ways to calculate beta are tested:1. over the whole year, 2. over last 60 days before winter onset.

#whole year
 obs<-n[,2]
obs<-as.data.frame(obs)
ys           <- nrow(obs)/366

# fill in gaps by interpolation
obs          <-na.interpolation(obs)

# append day
obs           <- cbind(obs,rep(1:366,ys))
colnames(obs) <- c("measurement","day")

# extract daily mean from each observations  
daily.average <- ave(obs$measurement,obs$day)
obs$norm      <- obs$measurement - daily.average

# spectral density
dens          <- spectrum(obs$norm,plot=F,method="pgram")
# select the interval [1/n,.5/month] (i.e. ignore frequencies below the resolution of the data and above a 2-month period)
freq          <- dens$freq[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]
spec          <-dens$spec[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]

# linear regression
if(length(freq)<5){next
  }else{
reg <- lm(log10(spec)~log10(freq))
beta_y[i]<--coef(reg)[2]
}

#2. 2 months
potentially_wrong<-function(){
  #idea: transpose and use gather to make 1 long vector ("day1:day 10,000" only that they are not called like that)
        #cut at (winter onset - 60) and winter onset of each year (multiple pieces)
        #clean incomplete data
        #calculate colour of noise on these chunks
  s<-s[w_on>67,]
  w_on<-w_on[w_on>67]
  s<-droplevels(s)
  if(length(w_on[w_on>60])==0){next}
  #transpose
  rownames(s)<-1:nrow(s) #these are years 1:x
  n<-gather(as.data.frame(t(s[3:368]))) #turns into long format with 2 columns:
  n<-droplevels(n)
  #key = 1,1,1...(366 times), 2,2,2...(366 times),..,40,40... (assuming there are 40 years)
  #value = temperature year 1, day 1:366, year 2, day 1.... 
  n$xvec<-rep(1:(ncol(s)-2),length(unique(n$key))) #xvec is days after 1st july, so it repeats 1:366, 40 times
  n$key<-as.integer(n$key)
  
  #cut
  n<-filter(n,xvec<w_on[key] & xvec >= w_on[key]-60)  #now dataset contains only last 60 days before winter onset in that respective year
  
  #clean
  n2<-group_by(n,key)
  sm<-summarise(n2,sum(!is.na(value))>40) #makes a tibble with 2 cols, key and usable==T or F
  usable<-pull(sm[sm[,2]==T,1]) #makes a vector with all years that have >10 days
  if(length(usable)<1){next}
  n<-n[n$key %in% usable,]

  obs<-n[,2]
obs<-as.data.frame(obs)
ys           <- nrow(obs)/60

# fill in gaps by interpolation
obs          <-na.interpolation(obs)

# append day
obs           <- cbind(obs,rep(1:60,ys))
colnames(obs) <- c("measurement","day")

# extract daily mean from each observations  
daily.average <- ave(obs$measurement,obs$day)
obs$norm      <- obs$measurement - daily.average

# spectral density
dens          <- spectrum(obs$norm,plot=F,method="pgram")
# select the interval [1/n,.5/month] (i.e. ignore frequencies below the resolution of the data and above a 2-month period)
freq          <- dens$freq[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]
spec          <-dens$spec[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]

if(length(freq)<5){next
  }else{
# linear regression
reg <- lm(log10(spec)~log10(freq))
beta_m[i]<--coef(reg)[2]
}

#abline(lm(log10(dens$spec)~log10(dens$freq)))
}


  } #next station


#####gather and save results####
reslist<-data.frame(unique(station$ID),mean_winter,sd_winter,nyears,p,A,phi,int,beta_y)

names(reslist)<-c("ID","meanwinter","sd winter","nyears","predictability","Amplitude","phase_angle","intercept","beta" )
write.table(reslist,"results.txt",append=TRUE,quote=FALSE,row.names=FALSE,col.names=FALSE)
} #next chunk
```


In the next step the results have to be combined with data from empirical studies. this is in another script (folder 03Analysis)


