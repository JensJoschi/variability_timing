---
title: "Calculation of winter variability: X days below Y °C"
author: "Jens Joschinski"
date: "February 1, 2018"
output: 
    md_document:
        variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
#setwd(dirname(dirname(dirname(getwd()))))
library(RCurl)
library(readr)
library(data.table)
library(textreadr)
library(tidyr)
library(dplyr)
library(stringr)
library(magrittr)
threshold<-50
t2<-10
```
# General description  
## Aim  
The aim of this project is to correlate climate variability with variability in seasonal timing. Is the slope in seasonal responses a bet-hedging trait, i.e., is it adaptive to spread one's timing in more variable conditions?  


### Overview  
The script uses the daily minimum and maximum temeratures for all months, years and climate stations all over the world from the GHCN dataste (~12million X 31 days).These were pre-processed with PERL and saved as two txt files. The two datasets are now opened, cleaned, and the mean of min and max is calculated for each day. The resulting workspace with one matrix of 11milx31days is saved.  
Further processing uses smaller chunks of data at a time (30 chunks of 1k climate stations). These are subsequently read in and transformed to differen format (so that 1 row has temperature of a whole year). Then I calculate winter onset as xth day that temperature falls below y°C. For each station I calculate mean winter onset (weighted by available data per year), standard deviation in winter onset, and winter predictability: I make a linear regression through the last 31 days before winter onset of each year, and calculate the slope. The inverse of the standard deviation of this slope estimate across years is winter predictability.Finally, all data of one station is put into a single vector, and I apply a non-linear least square regression: The data is expected to follow a sine-curve with a period of 365 days, and I record for each station amplitude (difference between summer and winter), phase angle (day of peak summer) and intercept (mean temperature). The results (mean winter onset, sd, correlation, nls-parameters) are saved in a text file. All of this is done on all chunks of data, and the results are appended in a single file.



### Specific description  

The data was generated with R version `r getRversion()`. It uses the GHCN-daily dataset by NOAA:
```{r, echo =F}
text<- read_document("01climate_data/01raw/ghcnd_all/ghcnd-version.txt")
print (text[1])
```


### Script  


#### Read climate station data and clean up 

Reading data and removing stations for which only mintemp or only maxtemp exists
```{r read_n_clean}
converted<-fread("01climate_data/02processing/001data_conversion/converted_max.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F)

length(unique(converted$V1))
conv_table<-table(converted$V1)
inlist<-conv_table[conv_table>=3] #removes anything with less than 3 years of data
converted<-converted[converted$V1 %in% names(inlist),]


converted2<-fread("01climate_data/02processing/001data_conversion/converted_min.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F)
conv_table2 <- table(converted2$V1)
inlist2<-conv_table2[conv_table2>=3]
converted2<-converted2[converted2$V1 %in% names(inlist2),]

#now in both files stations with <3 years are removed: 700 from max temps, 611 from min temps

converted2<-converted2[converted2$V1 %in% names(inlist),] 
#min temps consists now only of stations that are also in max temps
converted<-converted[converted$V1 %in% names(inlist2)]
#and max temps only has stations that are also in min temps
length(unique( converted$V1))
length(unique(converted2$V1)) #same
```

An older version of this script removed climate stations below 20°N or 18°N here. But some study sites are at ~ 18°N, and the interpolation to climate stations requires a 5° radius. to be sure, I now try at 0°N

```{r remove_under20}
url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
#this dataset is fixed-width delimited, requiring a few additional steps
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional separators
  widths=c(11, 9, 10, 7,2,35)
)

badlist <- locations[locations[,2]<0,1]  #latitudes smaller 0°N
rm(locations)
rm(url)
converted<-converted[!(converted$V1 %in% badlist),]
#removed 1,163,477 lines = 3,445 stations
converted2<-converted2[!(converted2$V1 %in% badlist),]
#removed 1,185,911 lines = 3,445 stations
rm(inlist)
rm(inlist2)
rm(conv_table)
rm(conv_table2)
rm(badlist)
rm(text)
```


removing any month that occurs in only one of the two datasets (nrow should be the same for both)

```{r remove_uniquemonths}
converted<-unite_(converted,"new_id",c("V1","V2","V3"),remove=FALSE)
converted2<-unite_(converted2,"new_id",c("V1","V2","V3"),remove=FALSE)


converted2<-converted2[converted2$new_id %in% converted$new_id]
converted<-converted[converted$new_id %in% converted2$new_id]
nrow(converted)
#we are now down to 11.4 million rows in each dataset

converted<-arrange(converted, new_id)
converted2<-arrange(converted2, new_id)


#calculate average Temperature:
#(max-min)/2 + min
converted[,5:ncol(converted)]<-(converted[,5:ncol(converted)]-converted2[,5:ncol(converted2)])/2+converted2[,5:ncol(converted2)]
rm(converted2)
converted<-converted[,-1]
names(converted)<-c("ID","year","month",1:31)
mymonths <- c("07Jan","08Feb","09Mar",
              "10Apr","11May","12Jun",
              "01Jul","02Aug","03Sep",
              "04Oct","05Nov","06Dec")
converted$month<-mymonths[converted$month]  #when it is sorted alphabetically it now starts in july, and ends in jun
rm(mymonths)
save.image(file = "converted.RData") #11754738 rows, 34 columns, 30901 stations
#row 1         ID year month   1     2     3   4     5     6   7   8   9    10  11  12  13  14  15
#1 ACW00011604 1949 07Jan 253 258.5 252.5 258 255.5 255.5 253 242 247 230.5 228 236 214 225 228
#     16  17    18  19    20    21    22  23  24    25  26    27    28  29  30    31
#1 244.5 239 241.5 233 244.5 247.5 244.5 239 239 233.5 250 244.5 241.5 247 239 244.5
```

#### data structure  
Due to problems with memory allocation, the big (26 million rows) dataset needs to be processed piece-wise. I chose 30 chunks of 1000 climate stations each. 
The following steps have to be done on that 1000 station dataset

* change data structure from 
    station 1 year 1 month 1 day 1,2,3..31
    station 1 year 1 month 2 day 1,2,3..31  
  to:  
    station 1 year 1 day 1,2,3.. 365
    station 1 year 2 day 1,2,3.. 365  
    
* make subset with 1 station and
  * calculate winter onset in each year  
  * calculate mean winter onset (weighted by number of days per year)  
  * calculate weighted standard deviation in winter onset  
  * calculate standard deviation in slopes of temperature decline before winter  
  * change structure to station 1 day 1....20,000 and make nls regression  

changing the data structure:
```{r transform_matrix}
turn_around <- function(station){
station<-
  station %>% 
  gather(key="day",value="temperature",-(1:3)) %>% #makes one very long dataframe with 2 columns
  mutate (day=str_pad(day,2,pad="0")) %>%  #this makes leading zeros where they are needed
  mutate (temperature=as.numeric(temperature)) %>%
  arrange (ID,year,month,day) %>%
  unite_("un",c("month","day")) %>%  #makes new col with new ID
  spread(key="un",value="temperature")#puts in new wide format with 365 columns (month-day)

#length(station$`08Feb_29`) #not many, due to leapyears
x<-NA
for(i in 1:ncol(station)){x[i]<-length(unique(station[,i]))}
station<-station[-which(x==1)]  #dates like 30 feb are erased
return(station)
}

```



### nls regression  (though that part wont be needed anymore)
The following chunk has the function to apply a nls regression on the climate data of each station. The daily temperatures over ~20 years are expected to follow a sine-curve pattern with a period of 1 year. The curve is determined by the following parameters:  
* a *constant c* that defines the average temperature throughout the year. It is around 10°C in temperate climates, around 25°C at the equator.  
* the *amplitude A*, which quantifies the difference between winter and summer temperatures. A should decrease with proximity to the poles.  
* the *phase angle phi*. Phi defines at what time of the year maximum temperatures occur. It should be close to midsummer in all stations of the northern hemisphere. june 25 is the julian day 176/177. This corresponds to a phase angle of 170/365 \*(2\*pi) = 2.92.

```{r nls_regression}
get_nls <- function (vals,s_A=400,s_phi=pi/2,s_c=200){#s_... are starting values for nls function
 if (is.na(length(vals))|| length(vals)<1000){return (NA)} else{ #when there is insufficient data, the function will return "NA"
  x<-1:length(vals)
  data<-data.frame(x,vals)

  #make nls regression for function 'y=Amplitude * cosine(period*x + phase angle) + intercept'
  res <- nls(vals ~ A*cos(x*2*pi/366+phi)+C, data=data, start=list(A=s_A,phi=s_phi,C=s_c),
            upper=c(500,2*pi,400),algorithm="port",lower=c(0,0,-400)) 
#366 days, non-leap-years have an NA at feb29

  return(coef(res))
  }
}


#wrapping function that allows continuing if errors occur in one of the stations
wrap_error = function (vals,s_A,s_phi,s_c) {
  tryCatch(get_nls(vals,s_A,s_phi,s_c),
                     warning = function (w)  {return(FALSE)},
                     error   = function (e)  {return(FALSE)})
}

```

### actual calculation  
Do the calculations, piecewise for 30 chunks of data; save the results
```{r}   
for (loop in 1:30){ #this loop goes through the 30 chunks of 1k stations
station<-converted[converted$ID %in% unique(converted$ID)[(loop-1)*1000+1:1000*loop],]
station<-turn_around(station)


mean_winter<-rep(NA,length(unique(station$ID))) #the results will be stored in  these vectors
sd_winter<- rep(NA,length(unique(station$ID)))
nyears<-rep(NA,length(unique(station$ID)))
p<-rep(NA,length(unique(station$ID)))
A<-rep(NA,length(unique(station$ID)))
phi<-rep(NA,length(unique(station$ID)))
int<-rep(NA,length(unique(station$ID)))


for (i in 1:length(unique(station$ID))){ #this loop goes through each of the 1 k stations
  
  ####read in station####
  id<-unique(station$ID)[i]
  s<-filter(station,ID==id)
  #format of the data: year 1, day 1...365
  #                    year 2, day 1...365
  
  #remove years with <182 days of data
  ndays<-rowSums(!is.na(s))-2 #number of days with data per year, -2 because need to substract id and year
  s<-s[ndays>182,3:ncol(s)]
  ndays<-ndays[ndays>182]
  
  #skip stations with <3 years
  if (nrow(s)<3) {
    mean_winter[i]<-NA
    sd_winter[i]<-NA
    next #next i, so code proceeds with next climate station
  }
  
  ####calculate winter arrival#####
 # make a binomial matrix with all Temps<threshold = 1, all others =0(including NA)
  bino<-s
  bino[bino<threshold]<-1
  bino[bino>=threshold]<-0
  bino[is.na(bino)]<-0
  rownames(bino)<-1:nrow(bino)
  
  #exclude years that never reach winter (less than x days below y°C)
  used<-rowSums(bino)>t2
  
  #exclude stations that reach winter in less than 3 years
  if (sum(used) <3) {
    mean_winter[i]<-NA
    sd_winter[i]<-NA
     next
  }
  
  bino<-bino[used,]
  s<-s[used,]
  w_on<-rep(NA,nrow(bino))
   
  #for each row calculate date when cumsum = t2 (winter)
   for (j in 1:nrow(bino)){
     w_on[j]<-min(which(cumsum(as.integer(bino[j,]))==t2))
   }

  ##calculate mean and sd in winter arrival (both weighted)####
  ndays<-ndays[used]
  mean_winter[i]<-weighted.mean(w_on,ndays)
  weights<-ndays/366
  upper <- sum(weights*(w_on-mean_winter[i])^2)
  lower <- sum(weights)-1
  sd_winter[i] <- sqrt(upper/lower)
  
  #clean up memory####
  rm(upper)
  rm(lower)
  rm(weights)
  rm(ndays)
  rm(bino)

  ##calculate standard deviation in slopes of last 30 days before winter onset####
 nyears[i] <- length(w_on)
  #idea: transpose and use gather to make 1 long vector ("day1:day 10,000" only that they are not called like that)
        #cut at (winter onset - 30) and winter onset of each year (multiple pieces)
        #clean incomplete data
        #make correlations of temperature ~ time to get the slope of each year
        #calculate sd of the slopes between years
  
  #transpose
  rownames(s)<-1:nrow(s) #these are years 1:x
  n<-gather(as.data.frame(t(s))) #turns into long format with 2 columns:
  #key = 1,1,1...(366 times), 2,2,2...(366 times),..,40,40... (assuming there are 40 years)
  #value = temperature year 1, day 1:366, year 2, day 1.... 
  n$xvec<-rep(1:ncol(s),length(unique(n$key))) #xvec is days after 1st july, so it repeats 1:366, 40 times
  n$key<-as.integer(n$key)
  
  #cut
  n<-filter(n,xvec<w_on[key] & xvec >= w_on[key]-31)  #now dataset contains only last 31 days before winter onset in that respective year
  
  #clean
  n2<-group_by(n,key)
  sm<-summarise(n2,sum(!is.na(value))>10) #makes a tibble with 2 cols, key and usable==T or F
  usable<-pull(sm[sm[,2]==T,1]) #makes a vector with all years that have >10 days
  n<-n[n$key %in% usable,]

  #slope estimates
  slopes<-rep(NA,length(usable))
  for (keygroup in usable){ 
    y<-filter(n,key == keygroup)
    slopes[keygroup]<-coef(lm(y$value~y$xvec))[2]
  }
  #translation: for each year take data of that year, save slope estimate from lm
  p[i]<-sd(slopes,na.rm=T)

  
  #make non-linear least-square regression ####
# The function tries up to three times per climate station, with different starting parameters.
 n<-gather(as.data.frame(t(s)))
 parms<-wrap_error(n$value,s_A=400,s_phi=pi/2,s_c=200)
  if (parms[1]==FALSE){parms<-wrap_error(n$value,s_A=40,s_phi=pi,s_c=200)}
  if (parms[1]==FALSE){parms<-wrap_error(n$value,s_A=40,s_phi=2*pi,s_c=200)}
  if (parms[1]==FALSE){print (paste("failed in ",i));parms<-c(-9999,-9999,-9999)}

 A[i]<-parms[1]
 phi[i]<-parms[2]
 int[i]<-parms[3]
 
  } #next station


#####gather and save results####
reslist<-data.frame(unique(station$ID),mean_winter,sd_winter,nyears,p,A,phi,int)

names(reslist)<-c("ID","meanwinter","sd winter","nyears","predictability","amplitude","phase angle", "intercept")
write.table(reslist,"results.txt",append=TRUE,quote=FALSE,row.names=FALSE,col.names=FALSE)
} #next chunk
```


This goes only for the first 30k climate stations! repeat loop with remaining stations!

Results.txt has been manually moved to the folder 01climate_data/03output.

In the next step the results have to be combined with data from empirical studies. this is in another script (folder 03Analysis)

Reading the results:
* histograms of all variables
* truncate sd for better plotting
* plot mean,sd,predictability vs lat+lon+alt
* plot resids of that model
* plot mean,sd,pred vs amplitude
* plot coefs on real topographic map

```{r}
reslist<-read.table("results.txt")
names(reslist)<-c("ID","meanwinter","sd_winter","nyears","predictability","amplitude","phase angle", "intercept")
r2<-merge(locations,reslist,by=1)

names(r2)<-c("ID","lat","lon","alt","no_idea","name","meanwinter","sd_winter", "nyears","unpredictability","amplitude","phase angle", "intercept")


#a few histograms to check that the things make sense
hist(r2$alt)
r2$alt[r2$alt==-999.9]<-NA
r2$sq_alt <-sqrt(r2$alt)
hist(r2$sq_alt,breaks=100)

hist(r2$meanwinter,breaks=100)
hist(r2$sd_winter,breaks=100)
r2$capped_sd<-r2$sd_winter
r2$capped_sd[r2$capped_sd>50]<-50

hist(r2$nyears,breaks=50)

hist(r2$unpredictability,breaks=100)
sum(!is.na(r2$unpredictability))
sum(is.na(r2$unpredictability))
length(r2$unpredictability[r2$unpredictability>5])-3911
97/16923 #0.5% of all values are >5, 1.1%>4
hist(r2$unpredictability[r2$unpredictability<5],breaks=100)


p<-r2

hist(p$amplitude)
hist(p$`phase angle`)
hist(p$intercept)



#checking whether the nls regression makes sense

p$intercept[p$intercept<(-100)]<-(-100)
p$intercept[p$intercept>200]<-200
p$intercept<-p$intercept+abs(min(p$intercept))
p$amplitude[p$amplitude>250]<-250
plot(p$lat~p$lon,bg = rgb(p$intercept,0,p$intercept,maxColorValue = max(p$intercept)),col=NA,pch=22,cex=0.3)
plot(p$lat~p$lon,bg = rgb(p$amp,0,p$amp,maxColorValue = max(p$amp)),cex=0.3,pch=22,col=NA)
plot(p$lat~p$lon,bg=rgb(p$`phase angle`,0,p$`phase angle`,maxColorValue = max(p$`phase angle`)),cex=0.3,pch=22,col=NA)


#visualisation mean winter, sd(winter), predictability
png("mean-winter.png")
p<-r2
p<-p[!is.na(p$meanwinter),]
plot(p$lat~p$lon,bg = rgb(p$meanwinter,p$meanwinter,0,maxColorValue = max(p$meanwinter)),cex=0.3,pch=22,col=NA, main ="mean winter onset",xlab="",ylab="")
dev.off()

p<-r2
p<-p[!is.na(p$capped_sd),]
p$capped_sd[p$capped_sd>20]<-20
p<-p[p$nyears>8,]
png("sd-winter.png")
plot(p$lat~p$lon,bg = rgb(p$capped_sd,p$capped_sd,0,maxColorValue = max(p$capped_sd)),cex=0.3,pch=22,col=NA, main ="sd winter onset, capped at 20",xlab="",ylab="")
dev.off()

p<-p[!is.na(p$sq_alt),]
M<-lm(p$sd_winter~p$lat+p$sq_alt)
summary(M)#R²=0

p<-r2
p<-p[!is.na(p$unpredictability),]
p<-p[p$nyears>5,]
p$unpredictability[p$unpredictability>5]<-5 #0.5% of all data
range(p$unpredictability)
png("predictability-winter.png")
plot(p$lat~p$lon,bg = rgb(p$unpredictability,p$unpredictability,0,maxColorValue = max(p$unpredictability)),cex=0.3,pch=22,col=NA, main ="unpredictability",sub="dark low standard deviation in slopes",xlab="",ylab="")
dev.off()



#how strongly does winter decrease per ° latitude? should be around 1h per 5°N
#though some conversion is needed
#with migration to north both mean winter and amplitude in day length change, makkng it a bit more complicated
p<-r2
plot(p$meanwinter~p$lat,pch=22,cex=0.1)
abline(lm(p$meanwinter~p$lat),col=2)
summary(lm(p$meanwinter~p$lat))
#mean winter decreases by 2.5 days per ° latitude
#=12.5 days per 5°. 
#so an organism needs to react to a day length equal to 12.5 days earlier in the year. what day length is that? well, depends on time of year and latitude.

#calculate day length 30° north at winter onset, then day length 35°N at winter onset - 12.5 days, and so on. this should be the critical day lenth for most organisms


M<-lm(p$meanwinter~p$lat)
182+coef(M)[1] + coef(M)[2]*c(30,35,40,45,50,55,60,65,70)
#this is mean winter onset at latitudes 30 - 70°N (182 is july 1st, a year was defined to begin on this date)

#it is equal to 13 sep - 23 dec 

#table for different day lengths
#30 °N, day 357(dec 23), 10:13 
#35°N, day 345(dec 11), 9:51
#40°N, day 332(28 nov), 9:37
#45°N, 319(15 nov), 9:34
#50°N, 307(3 nov), 9:41
#55°N, 294 (21 oct), 10:06
#60°N, 281 (8 oct), 10:50
#65°N, 269 (26 sep),11:53
#70°N, 256 (13 sep), 9:23
#so day length at winter onset changes nonlinearly with latitude

#actually, organisms need to prepare in advance, lets say two weeks
#30°N, day 343 (dec 9), 10:16
#35°N, day 331 (Nov 27), 10:03
#40°N, day 318 (nov 14), 10:01
#45°N, day 305 (nov 1), 10:10
#50°N, day 293 (oct 20), 10:29
#55°N, day 280 (oct 7), 11:07
#60°N, day 267 (sep 24), 12:05
#65°N, day 255 (sep 12), 13:26
#70°N, day 242 (Aug 30), 15:46

#according to literature the critical day length decreases by 1 -1.5 hours in photoperiod per 5°, but own data shows that it is more of an exponential function:
#needs to load slopes.txt

slopes$e[slopes$e>25]<-NA
mean(slopes$e,na.rm=T)
plot(slopes$e~slopes$lat)
points(x= c(30,35,40,45,50,55,60,65,70),y = c(10.27,10.05,10.02,10.17,10.48,11.12,12.08,13.43,15.77),col=2,pch=22,bg=2) #winter onset 2 weeks before
points(x= c(30,35,40,45,50,55,60,65,70),y = c(10.21,9.85,9.62,9.57,9.68,10.1,10.83,11.88,9.38),col=4,pch=22,bg=4)#winter onset
#general shape seems okay, only shifted by 2 hours day length. but these curves shift depending on temperature, so maybe lab temperatures are generally too cold? also the winter onset is quite arbitrary (could test with different thresholds (x and y))

#also test 3 weeks before just out of curiousity


summary(lm(p$unpredictability~p$lat+p$lon+p$sq_alt))
summary(lm(p$unpredictability~p$lat+p$lon+p$alt))
summary(lm(p$unpredictability~p$amplitude))
r<-resid(lm(p$unpredictability~p$sq_alt))

r<-r+abs(min(r))
r[r>4]<-4

plot(p$lat~p$lon,bg = rgb(r,r,0,maxColorValue = max(r)),cex=0.3,pch=22,col=NA, main ="resid predictability(capped)")
p<-p[p$lon<(-50),]
plot(p$lat~p$lon,bg = rgb(r,r,0,maxColorValue = max(r)),cex=0.5,pch=22,col=NA, main ="resid predictability(capped)")


```


####making model predictions based on lat,lon, alt, and interpolating using a topographic map of the world

####testing the topographic map
```{r}

r2$alt[r2$alt == - 999.9] <- NA
r2$alt[r2$alt<0]<-0
hist(r2$alt)
r2<-r2[is.na(r2$alt)==FALSE,]
plot(r2$lat~r2$lon,bg= rgb(0,r2$alt,0,maxColorValue = max(r2$alt)),cex=0.4,pch=22,col=NA)
points (-80.224167, 25.787778 ,col=2,cex=2,xpd=T) #miami
points (-81.784031, 24.559167 ,col=2,cex=2,xpd=T) #key west
points (-9.166667, 38.716667 ,col=2,cex=2,xpd=T)   #lissabon
points (141.700881, 45.39757 ,col=2,cex=2,xpd=T)  #wakkanai
#these points seem to be correct



data(ETOPO5)
#this topographic data is a matrix with 4320 rows (longitude) and 2160 cols (latitude), so the world is split into a 5' grid. But how is the data sorted?

N<-ETOPO5[,1081:2160] #second half of the dataset
N[N<0]<-0
N[N>1]<-1
image(N) # northern hemisphere, sorting is from +90 downwards

N<-ETOPO5[1:2160,1:1080]
N[N<0]<-0
N[N>1]<-1
image(N)
#sorting is from 0-360 ° (Erope->asia->USA)

#let's try to get a map of japan
#lat: 20.5:45.5
#lon: 122:142

#90°N should be at position 0 (column 0)
#89°N should be at (90-89) * 12 = 12
#0°N should be at 90*12 = 1080

(90-45.5)*12 #northern limit of japan
#this is about halfway through the norhtern hemisphere (2160/4)
(90-20.5)*12 #southern limit of japan
128.84*12 #goto islands, western limit
145.582*12 #eastern limit, nemuro (hokkaido)

N<-ETOPO5[1564:1747,534:834]
N[N<0]<-0
N[N>1]<-1
image(N)

#geting the altitude around wakkanai: 141.700881, 45.39757
(90-45.397)*12
141.700*12
ETOPO5[1690:1710,530:540]

#okay, seems to work


#what about locations at <0°E (e.g. US)
#making a map of florida
#21:30°N, -90:-77°E

12*(90-21)
12*(90-30)
12*(360-90)
12*(360-77)
t<-ETOPO5[3240:3396,720:828]
t[t<0]<-0
t[t>0]<-1
image(t)
##works

#comparing altitudes stated in r2 with altitudes from ETOPO5
r2$lon2<-r2$lon#convert negative longitudes to positives
r2$lon2[r2$lon2<0]<-360+r2$lon2[r2$lon2<0]

temp<-r2
temp<-temp[is.na(temp$cor)==F,]
nrow(temp)
temp$alt2<-NA
for(i in 1:nrow(temp)){
  temp$alt2[i]<-ETOPO5[round(temp[i,14]*12,0),round((90-temp[i,2])*12,0)]
}
plot(temp$alt~temp$alt2)

#the same approach can now be used to infer the altitude of the study sites (so there needs to be no station nearby)

```


```{r}
slopes<-read.table('/media/jens/296AA37A395D5FE6/g01-metaanalysis/02studies/01extracted_data/slopes.txt' ,header=T)
str(slopes)
names(slopes)<-c("name", "b","b_rel", "c" ,"c_rel", "d", "d_rel", "e" ,  "e_rel" ,"lat", "lon" ) #"rel" is for reliability, 1/s.e.
slopes<-slopes[-1,]
plot(slopes$lat~slopes$lon)
hist(slopes$lon,breaks=100)

slopes$alt<-NA
slopes$lon2<-slopes$lon
slopes$lon2[slopes$lon2<0]<-360+slopes$lon2[slopes$lon2<0]
for(i in 1:nrow(slopes)){
  slopes$alt[i]<-ETOPO5[round(slopes[i,13]*12,0),round((90-slopes[i,10])*12,0)]
}
hist(slopes$alt)

refmap<-r2
refmap$alt[refmap$alt<0]<-0
plot(refmap$lat~refmap$lon,bg = rgb(refmap$alt,0,refmap$alt,maxColorValue = max(refmap$alt)),col=NA,pch=22,cex=0.5)
points(slopes$lon,slopes$lat,col=2,cex=1.5)

N<-ETOPO5[1564:1747,534:834]
N[N<0]<-0
plot(NA,xlim=c(1,200),ylim = c(1,200))
for ( i in ncol(N):1){
  for (j in 1:nrow(N)){
    points(j,i,bg=rgb(0,N[j,i],0,maxColorValue = max(N)),pch=22,cex=0.4,col=NA)
  }
}
#make map of only amami

plot(NA,xlim=c(1564,1747),ylim=c(534,834))
points(N,bg=)
```


#now slopes has lat,lon and alt, and the slopes; climate data has lat,lon alt, and variability. modelling!

```{r}
r2$alt[r2$alt==-999.9]<-NA

p<-r2[!is.na(r2$cor),]
p$alt[p$alt<0]
p$alt[p$alt<0]<-0
p$sq_alt <-sqrt(p$alt)

M<-lm(p$cor~p$lat+p$lon+p$sq_alt)
summary(M)

p$preds<-coef(M)[1] + coef(M)[2] * p[,2] + coef(M)[3]*p[,3] + coef(M)[4] * p[,14]

length(p[is.na(p$preds),])
p<-p[!is.na(p$preds),]
p$preds<-p$preds+abs(min(p$preds))
png("predicted_predictability.png")
plot(p$lat~p$lon,bg=rgb(p$preds,p$preds,0,maxColorValue = max(p$preds)),pch=22,cex=0.4,col=NA,main ="predicted predictability, using lat+lon+alt")
dev.off()

#now, using that on lat lon and alt of slopes
head(slopes)
slopes$sqalt<-slopes$alt
slopes$sqalt[slopes$alt<0]<-0
slopes$sqalt<-sqrt(slopes$sqalt)
slopes$preds<-coef(M)[1] + coef(M)[2] * slopes$lat + coef(M)[3]*slopes$lon + coef(M)[4] * slopes$sqalt
#slopes$preds<-slopes$preds+abs(min(slopes$preds))
#plot(slopes$lat~slopes$lon,bg=rgb(slopes$preds,slopes$preds,0,maxColorValue = max(slopes$preds)),pch=22,cex=0.4,col=NA,main ="prediction based on slopes dataset, using lat+lon+alt")

slopes$b2<-abs(slopes$b)
slopes<-separate(slopes,col=1,into=c("study","pop"),remove=FALSE,extra="merge",sep="-")
slopes$study[slopes$study=="6b"]<-6
png("predictability_vs_slope.png")
plot(slopes$preds,slopes$b2,main = "environmental predictability vs diapause slopes",xlab = "environment", ylab = "physiology",pch=22,bg=slopes$study)
dev.off()
plot(slopes$preds,slopes$e,main = "environmental predictability vs cdl",xlab = "environment", ylab = "cdl")

summary(lm(b2~preds, weights=sqrt(b_rel/max(b_rel)),data=slopes))
library(lme4)
M2<-lmer(b2~preds+(1|study),weights=sqrt(b_rel/max(b_rel)),data=slopes)
summary(M2)





data(ETOPO5)
N<-ETOPO5[,1:1080]
for (i in 1:nrow(N)){
  for (j in 1:ncol(N)){
    if (N[i,j]<0) N[i,j]<-NA
  }
}

N<-N[,ncol(N):1]
#image(N)
#okay N is a topographic map of the world

#turning that strange matrix into a dataframe with 3 cols: lat,lon,alt
latvec<-(1:ncol(N)) * (90-20)/1080 #-20
lonvec<-(1:nrow(N)) * 360/nrow(N)
#image(lonvec,latvec,N)
s<-rep(latvec,each=nrow(N))
lons<-s
a<-as.numeric(N)
lats<-rep(lonvec,times=ncol(N))
lats<-lats[!is.na(a)]
lons<-lons[!is.na(a)]
a<-a[!is.na(a)]
#lats and longs are wrong
t<-lons
lons<-lats
lats<-t
rm(t)
rm(ETOPO5)
rm(locations)

rm(n)
rm(r)
#plot(lats~lons,bg=rgb(a,a,0,maxColorValue = max(a)),pch=22,cex=0.2,col=NA)
#looks like it worked well, only should be centered at 0°E

da<-data.frame(lats,lons,sqrt(a))
rm(lats)
rm(lons)
da<-da[da$lats<70,]
da$lons[da$lons<180]<- da$lons[da$lons<180]+360
da$lons<-da$lons-360
plot(da$lats~da$lons,cex=0.1)
names(da)=c("lat","lon","sq_alt")
p<-r2[!is.na(r2$cor),]
M<-lm(p$cor~p$lat+p$lon+p$sq_alt)
summary(M)
#predictions<-predict(M,newdata=da)
da$preds<-coef(M)[1] + coef(M)[2] * da[,1] + coef(M)[3]*da[,2] + coef(M)[4] * da[,3]
da<-da[!is.na(da$preds),]
da$preds<-da$preds+abs(min(da$preds))
plot(da[,1]~da[,2],bg=rgb(da$preds,da$preds,0,maxColorValue = max(da$preds)),pch=22,cex=0.1,col=NA,main ="predicted predictability, using lat+lon+alt")

#us only
us<-da[da$lon<(-50),]
plot(us[,1]~us[,2],bg=rgb(us$preds,us$preds,0,maxColorValue = max(us$preds)),pch=22,cex=0.1,col=NA,main ="predicted predictability, using lat+lon+alt")
```


correlating slpes from studies with this prediction -preliminary-
```{r}
names(da)
head(da)
slopes<-read.table(file.choose())
str(slopes)
da2<-da
names(slopes)<-c("name", "b","b_rel", "c" ,"c_rel", "d", "d_rel", "e" ,  "e_rel" ,"lat", "lon" )
slopes$lat <- round(slopes$lat,1)
slopes$lon<-round(slopes$lon,1)
da2$lat<-round(da2$lat,1)
da2$lon<-round(da2$lon,1)
da2<-unite(da2,"latlon",c("lat","lon"))
slopes<-unite(slopes,"latlon",c("lat","lon"))
together<-inner_join(slopes,da2)#not all found because capped at 60°N
together<-separate(together,"latlon",c("lat","lon"),"_" )


together<-separate(together,col=1, into = c("study","pop"), sep ="-",remove=FALSE)
together$lat<-as.numeric(together$lat)

together$study[together$study=="6b"]<-6
together$study<-as.integer(together$study)
together$e_rel2<-together$e_rel #backup
together$e_rel[together$e_rel>3000]<-3000
together$e_rel<-sqrt(together$e_rel)/max(sqrt(together$e_rel),na.rm=T)
plot(together$e~together$lat)
plot(da$lat~da$lon,bg=rgb(da$preds,da$preds,0,maxColorValue = max(da$preds)),pch=22,cex=0.1,col=NA,main ="predicted predictability, using lat+lon+alt")
points(together$lat~together$lon,bg=rgb(together$preds,together$preds,0,maxColorValue = max(together$preds)),pch=21)
plot(together$b~together$preds,cex=together$e_rel+0.5,bg=together$study,pch=21)
M<-lm(together$b~together$preds,weights=together$e_rel2)
summary(M)
abline(M)
library(lme4)
M2<-lmer(b~preds+(1|study),weights=e_rel2,data=together)
summary(M2)
together$p<-predict(M2)
plot(together$b~together$preds,cex=together$e_rel+0.5,bg=together$study,pch=21)

```

```{r}



#making a map of the world and marking the coordinates where I have data
plot("ETOPO5")

ETOPOn<-ETOPO5
ETOPOn[ETOPOn<0]<-0
#plot(NA,xlim=c(0,360*12),ylim = c(0,180*12))
#for ( i in 1:ncol(ETOPOn)){
#  for (j in 1:nrow(ETOPOn)){
 #   points(ETOPOn[j,i])
  }
}
points(ETOPOn,col=rgb(0,ETOPOn,0,maxColorValue = max(ETOPOn)))

points(slopes$lat~slopes$lon,col=2)



slopes<-read.table(file.choose())
str(slopes)
head(slopes)
nrow(slopes)
names(slopes)
slopes$lat5<-round((90+slopes$degN) * 12 ,0)
slopes$lon5<-round((90+slopes$degE) * 12 ,0) #convert negative to positive numbers
slopes$degE[slopes$degE<0]<-360-abs(slopes$degE[])


#take etopo5
#make data frame 
rownames(ETOPO5)
rownames(ETOPO5)<-attr(ETOPO5,"lon")
colnames(ETOPO5)<-attr(ETOPO5,"lat")

ETOPO5<-as.data.frame(ETOPO5)
gather(ETOPO5)
#lat lon alt
#make pred based on climate data naalysis
#plot data frame, colr = pred
#add points: slopes.txt coordinates
#merge slopes and preds (keep all)
#plot and add points again to verify
#plot preds vs slopes
```


```{r}

r2<-r2[!is.na(r2$cor),]
r2$sqy<-sqrt(r2$nyears)
plot(r2$lat~r2$lon,bg=rgb(r2$sqy,0,0,maxColorValue = max(r2$sqy)),pch=22,cex=0.3,col=NA)

r2$cv <- 1
r2$cv[r2$nyears<10]<-3
r2$cv[r2$nyears>50] <-2
plot(r2$lat~r2$lon,pch=22,bg= r2$cv,col=NA,cex=0.3,main="sample sizes",sub="<10y: green;10 to 50: black,>50:red")
```

WARNING
everyhting below is junk (i think)


The following chunk has the function to apply a nls regression on the climate data of each station. The daily temperatures over ~20 years are expected to follow a sine-curve pattern with a period of 1 year. The curve is determined by the following parameters:  
* a *constant c* that defines the average temperature throughout the year. It is around 20°C in temperate climates, around 30°C at the equator.  
* the *amplitude A*, which quantifies the difference between winter and summer temperatures. A should decrease with proximity to the poles.  
* the *phase angle phi*. Phi defines at what time of the year maximum temperatures occur. It should be close to midsummer in all stations of the northern hemisphere. june 25 is the julian day 176/177. This corresponds to a phase angle of 170/365 \*(2\*pi) = 2.92.

```{r nls_regression}
get_nls <- function (vals,s_A=400,s_phi=pi/2,s_c=200){#s_... are starting values for nls function
  if (is.na(length(vals))|| length(vals)<1000){return (NA)} else{ #when there is insufficient data, the function will return "NA"
  x<-1:length(vals)
  data<-data.frame(x,vals)

  #make nls regression for function 'y=Amplitude * cosine(period*x + phase angle) + intercept'
  res <- nls(vals ~ A*cos(x*2*pi/372+phi)+C, data=data, start=list(A=s_A,phi=s_phi,C=s_c),
            upper=c(500,2*pi,400),algorithm="port",lower=c(0,0,-400)) 
  #372 and not 365.25, because a vector of 12*31 was used before 
  #(filling up e.g. 31.feb with NA). This made coping with leap years easier

  return(coef(res))
  }
}


#wrapping function that allows continuing if errors occur in one of the stations
wrap_error = function (vals,s_A,s_phi,s_c) {
  tryCatch(get_nls(vals,s_A,s_phi,s_c),
                     warning = function (w)  {return(FALSE)},
                     error   = function (e)  {return(FALSE)})
}

```

```{r vectorize}
daily_t <- function (station){
  station<-station[arrange(station[,2],station[,3],na.last=F),] #sorts by year and month
  station<-droplevels(station)
  if (length(unique(station$year))<3){vals<- (NA)} #quality control
  else {
    vals<-NA
    #for each year:
    for(y in 1:length(unique(station$year))){
      year<-station[station[,2]==unique(station[,2])[y],] #reduces dataset to 1 station, 1 year          (max 31*12 points)
      
      #make 12*31 matrix and fill it with daily data
      mat <- matrix(NA,12,31)
  
      for (i in 1:nrow(year)){ #i=month
        mon<-as.numeric(year[i,3])
        mat[mon,1:31]<-as.numeric(year[i,4:34])
      }
    vals<-c(vals,as.numeric(t(mat)))
    }
  return (vals)
  }
}


```



####Function for nonlinear least-square regression  

The function 'get_nls' takes the daily temperatures of one station (which is supplied as single vector), and applies a non-linear least squares model which estimates intercept, phase angle and amplitude of a sine curve.




####applying the functions on the dataset

The following chunk will apply a nls regression on the climate data of each station. The daily temperatures over ~20 years are expected to follow a sine-curve pattern with a period of 1 year. The curve is determined by the following parameters:  
* a *constant c* that defines the average temperature throughout the year. It is around 20°C in temperate climates, around 30°C at the equator.  
* the *amplitude A*, which quantifies the difference between winter and summer temperatures. A should decrease with proximity to the poles.  
* the *phase angle phi*. Phi defines at what time of the year maximum temperatures occur. It should be close to midsummer in all stations of the northern hemisphere, and around midwinter in the southern hemisphere. june 25 is the julian day 176/177. BEcause the function daily_t assumes 12*31 = 372 days, it is around day 170 in this dataset. This corresponds to a phase angle of 170/372 \*(2\*pi) = 2.87.  The phase angle of midwinter is accordingly, 368/372\*(2\*pi)=6.22.

The function tries up to three times per climate station, with different starting parameters.

```{r quality-control}
#,error=TRUE}
phi=NA
A=NA
C=NA
lat=NA
lon=NA
reg<-data.frame(lon,lat,A,phi,C)

wrap_error = function (vals,s_A,s_phi,s_c) {
  tryCatch(get_nls(vals,s_A,s_phi,s_c),
                     warning = function (w)  {return(FALSE)},
                     error   = function (e)  {return(FALSE)})
}

for (i in 1:length(unique(testset$ID))){ #= for each climate station

temp_set<-testset[testset$ID==unique(testset$ID)[i],]
vals<-daily_t(temp_set)
if (length(unique(vals))>100){ #insufficient number of years in functon daily_t will make length (vals)=1; but it is still possible that there are 3+ years filled with NA (less than 100 data points)

  parms<-wrap_error(vals,s_A=400,s_phi=pi/2,s_c=200)
  if (parms[1]==FALSE){parms<-wrap_error(vals,s_A=40,s_phi=pi,s_c=200)}#print ("retry1");
  if (parms[1]==FALSE){parms<-wrap_error(vals,s_A=40,s_phi=2*pi,s_c=200)}#print ("retry2");
  if (parms[1]==FALSE){print (paste("failed in ",i));parms<-c(-9999,-9999,-9999)}

} else  {
  parms<-rep(NA,5)
}
reg[i,1]<-temp_set[1,35] #longitude
reg[i,2]<-temp_set[1,36]# latitude
reg[i,3]<-parms[1] # Amplitude
reg[i,4]<-parms[2] #phase angle
reg[i,5]<-parms[3]#intercept
reg[i,6]<-temp_set[1,1]#ID
}
reg<-reg[is.na(reg[,3])==F,]
```


####Plotting the results:

1. average Temperature
```{r T_avg}
restore <- reg
reg$C[reg$C<(-200)]<-(-200) #caps some very low temperatures, makes plotting gradient nicer

plot(reg[,1]~reg[,2],bg=rgb(reg[,5]+abs(min(reg[,5])),1,reg[,5]+abs(min(reg[,5])),maxColorValue = max(reg[,5])+abs(min(reg[,5]))),pch=21,col=NA,cex=0.5,
     main = "Average temperature(C)",xlab="°E",ylab="°N")
for (i in seq(min(reg[,5]),max(reg[,5]),1)){
  points(x=-150 + i/10,y = 0, bg= rgb(i+abs(min(reg[,5])),1,i+abs(min(reg[,5])),maxColorValue = max(reg[,5])+abs(min(reg[,5]))),col=NA,pch=22)
}
text(-160,-5,min(reg$C)/10)
text(-110,-5,max(reg$C)/10)
#hist(reg[,5],breaks=100)
reg<-restore
#mean Temperature behaves as expected
```


2. temperature amplitude
```{r t_amplitude}
restore <- reg
reg$A[reg$A>150]<-150 #caps some very high amplitudes
plot(reg[,1]~reg[,2],bg=rgb(reg[,3],1,reg[,3],maxColorValue = max(reg[,3])),pch=21,col=NA,cex=0.5,
     main = "Amplitude(A)",xlab="°E",ylab="°N")
for (i in seq(0,max(reg[,3]),1)){
  points(x=-150 + i/5,y = 0,bg=rgb(i,1,i,maxColorValue = max(reg[,3])),col=NA,pch=22)
}#left: A = 0, right: A = 24°
text(-150,-5,0)
text(-120,-5,max(reg[,3]/10))

#hist(reg[,3])

reg<-restore
#amplitude is 0 at equator, as expected.
#amplitude increases northwards to >15° diff in summer-winter
#In south, there are only data points up to 20°S in this sample dataset, but the trend is the same
```


3. phase angle
```{r phase_angle}
plot(reg[,1]~reg[,2],bg=rgb(reg[,4],1,reg[,4],maxColorValue = 2*pi),pch=21,col=NA,cex=0.5,
        main = "Phase angle",xlab="°E",ylab="°N")
for (i in seq(0,2*pi,0.1)){
  points(x=-150 + i*5,y = 0,bg=rgb(i,1,i,maxColorValue = 2*pi),col=NA,pch=22)
}#left: phi = 0, right: phi=2*pi
#hist(reg[,4])
#Due to the circular nature of the dataset, phase angles of phi=0 and phi=2*pi are equal. 
#better representation

plot(reg[,1]~reg[,2],bg=rgb(cos(reg[,4])+1,1,cos(reg[,4])+1,maxColorValue = 2),pch=21,col=NA,
       cex=0.5, main = "Phase angle",xlab="°E",ylab="°N")
for (i in seq(0,2*pi,0.1)){
  points(x=-150 + i*5,y = 0,bg=rgb(cos(i)+1,1,cos(i)+1,maxColorValue = 2),col=NA,pch=22)
}

#hist(cos(reg[,4]))

#phase angles behave as expected. mean of phi in the northern hemisphere:
mean(reg[,4][reg[,1]>20]) #this is shortly before midsummer
```


According to the subset, the data behaves just as expected.   
* amplitude decreases towards equator  
* amplitude is lower (less seasonality) at coastlines (e.g. Australia, west coast USA)  
* mean temperature increases towards equator  
* mean temperature drops in mountain ranges  
* phase angle is close to midsummer in northern latitudes, and -180° in southern latitudes



The remaining part of the script will be done on northern hemisphere only (>20°N), as there is no empirical data for southern hemisphere.


##calculate winter variability

The location metadata has been erased in the meantime, because it takes quite a bit of memory. Reload it:

```{r get_coordinates_again,include=FALSE}
url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
#this dataset is fixed-width delimited, requiring a few additional steps
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional seps
  widths=c(11, 9, 10, 7,2,35)
)
names(locations) <- c("ID","lat","lon","alt","name","?")
```


A bit of cleaning...


```{r cleanup,echo=FALSE}
old<-testset #the full dataset, which includes equator and southern hemisphere; not needed anymore
newset<-old[old$lat>20,]
rm(testset)
gc()
testset<-newset[newset$ID==unique(newset$ID)[10],]
#now newset is the full dataset that is needed for analysis, testset a small subset for testing purposes
sfile<-unique(newset$ID)
```



####General idea

It is inconvenient that the year ends around midwinter. To calculate winter arrival, it makes more sense to have one "year" going from june to june:

```{r plot_idea}
plot(daily_t(testset),type="l")
#mean phase angle is at 2.78. 2.78/(2*pi)*372 =165 => highest temperature at day 165
points(x=165+372*(1:100),y= daily_t(testset)[165+372*(1:100)],col=2) #okay, dataset can be cut in this positiont
```

####How this is done

* process data from one station to get a single temperature vector (there is already a function for that, called daily_t)
* get rid of the first half year (which lies before the first red dot), and of the last half year (this does not contain full 12 months of information) 
* reassemble the temperature vector into a matrix, with each row representing 12 months (june to june) of data.
* For each year(row), count the number of days which have data
* calculate winter onset for each year:
   + make a binomial matrix, with all T<y = 1, all other T=0
   + calculate cumulative sums of each row
   + note the day where cum_sum > X
* calculate standard deviation and mean winter onset, weighted by number of days with data



####Function to cut and reassemble into binomial matrix

This function takes the looong temperature vector of one station (daily temperature of several years), cuts the head and tail (half a year each), puts it back into matrix format (1 row = 1 year), and transforms it into a binomial matrix


```{r get_binomial_matrix}
reassemble_cd <- function (station,degrees=50){ #method to calculate winter onset based on accumulation of days cooler than 50centidegree (5°C)
  #expected input: a vector of length 372*n. Should be of the form: for n years{temperatures from (Jan to Dec)}. 372, because the function daily_t uses 12*31 days for calculation (filling up with NAs)
  
  #error handling
  if (length(station[is.na(station)==F])<100){
    return (NA) #error if <100 days in total
  } else if ((length(station)-1)%%372 !=0 ){
    print("ERROR")
    return(NA) #error if there are incomplete years
  } else if (length(station)<=754){
    print ("ERROR")
    return(NA)#error if there are <3 years
  } else{
    
    #cutting and reassembling
    station <- station[2:length(station)] #function daily_t() has appended an NA to each year in the beggining
    nyears <- length(station)/372 #should be an integer
    station<-station[165:(length(station)-208)] #temperatures from first winter to summer are irrelevant, calculation begins with first summer; the last year ends in december, but data would be needed until june-->last year cannot be used for calculations
    
    
    #conversion to binomial
    station[station<degrees]<-1
    station[station>1]<-0
    m<-matrix(station,(nyears-1),372,byrow=T)
    return(m) #output: a matrix with 372 cols, n-1 rows(first half of first year, and 2nd half of last year removed)
  }
}

```



####Function to calculate number of days with data

The script will later calculate the mean winter onset at each station. Because data quality differs among years, the mean needs to be weighted by the number of days with data per year. To do that, one needs to actually have the number of days:


```{r, count_NAs}
row_na<-function (inp_matrix){
  navec<-NA #this means na_vector
  if (is.matrix(inp_matrix)==TRUE){
    navec <- rowSums(!is.na(inp_matrix))
   # for (i in 1:nrow(inp_matrix)){
   #   navec[i]<-length(inp_matrix[i,][is.na(inp_matrix[i,]==TRUE)])
   # }
  }
  return(navec)
}
```


####Function to test whether the cumulative sum of occurences of y reaches x = 10

```{r cumulator}
calc_cumul <- function (inp_matrix,thres=10, conv_NA =TRUE){ #takes a matrix as input, calculates rowwise cumulative sums, and returns the position on which a threshold was first reached in each row 
  
  #input: any 2-dimensional matrix, a threshold
  #output: sum_vector
  
  #error handling
  if (is.matrix(inp_matrix)==FALSE){stop("no matrix provided in function calc_cumul")}
  if (conv_NA ==TRUE){
  vector<-is.na(inp_matrix)
  inp_matrix[vector]<-0
  }
  if (NA %in% inp_matrix){ #This part will only run if conv_NA ==F and there are nevertheless NA in the matrix
      stop("NA found in function calc_cumul. Try conv_NA=TRUE")
  }
  
  sum_vector <- rep(NA,nrow(inp_matrix))
  for (m_row in 1:nrow(inp_matrix)){
    temporary <-cumsum(inp_matrix[m_row,])
    if  (length(temporary[temporary==thres])>0){#only continue if threshold is reached
      sum_vector[m_row]<-min(which(temporary==thres))  #Cumsum increases 'from left to right' in the matrix. But if there are zeros in the original matrix, the cumsum may stay on one value, so threshold will occur several times in a row. the min() makes sure only the first value is taken 
    } #else{sum_vector stays NA}
  }
  return (sum_vector)
}
```


####Function that puts all these functions for one station together

```{r winter_one_station}
do_all<-function(station,x=10,y=50){
  temp_set<-newset[newset$ID==unique(newset$ID)[station],] #read all data from one station
  
  vals<-daily_t(temp_set)#convert data frame into single vector: (day 1:372)*years
 #this returns: either a matrix, or NA
  
  n_t<-reassemble_cd(vals,y)# convert into binomial matrix of form {1:372}*(years-1)
  #this returns either a matrix or NA, input NA => output NA
  
  ndays<-row_na(n_t) #this vector describes how many temperature measurements are available per year for this station
  
  if ((is.null(nrow(n_t))==F) && (nrow(n_t)>1)){#calculation for this station only if functions "temp_set" and "reassemble" did not return {NA}
    out<- calc_cumul(n_t,x) #vector describing on which day y was reached for the xth time
  #error if no matrix as input, or if NA provided and setting conv_NA ==F
  }else{
    out<-NA
  }
  return(list(out,ndays))
}

```



####Function to calculate weighted means

The built-in function cannot handle NA

```{r weighted_means}
weighted_means<-function(resm,nm){ #calculate row-wise means of a matrix, weighted by another matrix, and ignoring any NAs
  ms<-rep(NA, nrow(resm))
  for (i in 1:nrow(resm)){
    worked<-which(!is.na(resm[i,1:ncol(resm)]))
    if (length(worked)>3){
      ms[i]<-weighted.mean(resm[i,worked[worked>3]],nm[i,worked[worked>3]])
    }
  }
  return(ms)
}
``` 

####function to calculate standard deviation
```{r sd}
standard_dev<-function(resm){ #calculate row-wise sd of a matrix, ignoring any NAs
  ms<-rep(NA, nrow(resm))
  for (i in 1:nrow(resm)){
    worked<-which(!is.na(resm[i,1:ncol(resm)]))
    if (length(worked)>3){
      ms[i]<-sd(resm[i,worked[worked>3]])
    }
  }
  return(ms)
}
``` 


####wrapping function to run the code for all stations


```{r winter_all_Stations}
loop_stations<-function(x=10,y=50){
#create a matrix:  9000 rows (1 for each station), 140 columns (1 for each year)+3 columns for ID,latitude, longitude. Each cell has the winter onset in one year
resmat<-matrix(NA,length(unique(newset$ID)),max(newset$year)-min(newset$year)+3)
nmat  <-matrix(NA,length(unique(newset$ID)),max(newset$year)-min(newset$year)+3)
lat_lon<-locations[locations$ID %in% sfile,1:3]
#table(sfile==lat_lon[,1])#always true
resmat<-as.data.frame(resmat)
nmat<-as.data.frame(nmat)
resmat[,1]<-sfile
resmat[,2]<-lat_lon[,2]
resmat[,3]<-lat_lon[,3]
nmat[,1:3]<-resmat[,1:3]
#loop through all stations, calculate winter onset given x and y values
for (station in 1:length(sfile)){
  r<-do_all(station,x,y)
  r[[1]][r[[2]]<100]<-NA #remove all years with less than 100 days of data
  resmat[station,4:(length(r[[1]])+3)]<-r[[1]]
  nmat  [station,4:(length(r[[2]])+3)]<-r[[2]]
}

final<-resmat
final2<-nmat
return(list(final,final2))
}

```



####running the actual code

This chunk tries 4 different parameter combinations
```{r run_the_code}
final_10_50<-loop_stations(x=10,y=50)
final_5_20<-loop_stations(x=5,y=20)
final_20_100<-loop_stations(x=20,y=100)
final_5_0<-loop_stations(5,0)

```

*The results are stored in lists with 2 entries: final[[1]] is the matrix with winter onsets in each year (columns) and for each station (rows), final[[2]] is a matrix with the according sample size (number of days per year)*




####plotting results


The follwoing chunk plots mean winter onset (weighted by sample size within each year, i.e. days with data), then sd(winter onset), and lastly the coefficient of variation, sd/mean. This is done for four parameter combinations. 


```{r plot_the_results}

lis<-list(final_10_50,final_5_20,final_20_100,final_5_0)
x<-c(10,5,20,5)
y<-c(50,20,100,0)
CV<-list(NA,NA,NA,NA) # will be used in later chunk


for (i in 1:length(lis)){
  input <- lis[[i]]
  subtitle<-paste(x[i],"days below ", y[i], "°C") #will be needed in figures
  
  input[[1]]$means<-weighted_means(input[[1]],input[[2]])
  input[[1]]$sds<-standard_dev(input[[1]])
  input<-input[[1]]
  input<-input[!is.na(input$means),]
  plot(input[,2]~input[,3],bg=rgb(0,input$means,0,maxColorValue=372),col=NA,main="weighted mean day of winter onset",sub=subtitle,pch=22,cex=0.8)
   
  input$sds[!is.finite(input$sds)]<-NA
  input<-input[!is.na(input$sds),]
  input<-input[input$sds>0,]
  plot(input[,2]~input[,3],bg=rgb(0,input$sds,0,maxColorValue=max(input$sds)),col=NA,main="Sd of winter onset",sub=subtitle,pch=22,cex=0.8)
  
  input$CV<-input$sds/input$means
  input$CV[!is.finite(input$CV)]<-NA
  input<-input[!is.na(input$CV),]
  CV[[i]]<-as.data.frame(input$CV,input$V1)
  plot(input[,2]~input[,3],bg=rgb(0,input$CV,0,maxColorValue=max(input$CV)),col=NA,main="Sd/mean of winter onset",sub=subtitle,pch=22,cex=0.8)
}
```

```{r mins}
minesweep_r <- function (input_vector){
  mines_vector <- rep(0,1000)
  mines_vector[sample(1000,10)]<-1
  if (sum(mines_vector[input_vector])>0){print ("you lost")} else {print ("you won")}
}
minesweep_r(input_vector=sample(1000,990))
```


The results of the 4 runs are very similar, and the sd and CV plots are very dark. this is because the standard deviation is rather long-tailed, caused by the northernmost stations. Cropping it a bit to get a better view (last run only):

```{r plot_cropped}
cropped<-input
cropped$sds[cropped$sds>50]<-50
cropped$CV[cropped$CV>0.5]<-0.5
  plot(cropped[,2]~cropped[,3],bg=rgb(0,cropped$sds,0,maxColorValue=max(cropped$sds)),col=NA,main="Sd of winter onset",sub=subtitle,pch=22,cex=0.6)
  plot(input[,2]~input[,3],bg=rgb(0,cropped$CV,0,maxColorValue=max(cropped$CV)),col=NA,main="Sd/mean of winter onset",sub=subtitle,pch=22,cex=0.6)
```



####comparing the 4 runs

```{r comparison}
m<-merge(CV[[1]],CV[[2]],by=0) #by=0 merges by rownames
m2<-merge(CV[[3]],CV[[4]],by=0)
m<-merge(m,m2,by=1) # 1, because they have now a separate column with names
names(m) =c("ID","10_50","5_20","20_100","5_0")
plot(m[,2:5])

```

These look quite similar, only the parameter run "20 days below 10°C" sticks out as different. But this parameter combination is really quite different from the others. 


####weighted standard deviation  

The number of days with data is likely different among years. How is the overall data quality?


```{r ndays}
cut<-final_10_50[[2]][,4:(ncol(final_10_50[[1]]))] #removes the columns mean and sd
mark<-!is.na(cut)
test<-cut[mark]
hist(test,breaks=100,xlab="Days",ylab ="Frequency",main ="Frequency distribution: Number of days/year")
length(test[test<300])/length(test)
# 18% <300


```


While most years have complete data, a significant proportion (>20%) is incomplete. the between-years standard deviation hence needs to be weighed by the reliability of each year (sample size within year).  

####calculation of weighted sd

```{r calc_sdweighed}
weighted<-list(NA,NA,NA,NA)
for (i in 1:length(lis)){
  input <- lis[[i]] #lis has the 4 runs stored
  weighted[[i]]<-data.frame(NA,NA,NA)
  names(weighted[[i]])<-c("ID","m","sd")
  
  for (j in 1:nrow(input[[1]])){
    mean_w_on<-NA #will temporarily store mean winter onset of this station
    wsd <- NA     #same for weighted sd
    
    w_on<-input[[1]][j,4:(ncol(input[[1]])-3)] #a vector with winter onsets of each year
    weights<-input[[2]][j,4:(ncol(input[[2]])-3)] #deselect first 3 rows (ID,LAT,LON)
    
    #remove NAs
    w<-!is.na(w_on)
    weights<-weights[w]
    weights<-weights/372
    w_on <- w_on[w]
  
    #calculate row-wise weighted mean winter onset
    mean_w_on<-weighted.mean(w_on,weights)
    
    #calculate row-wise weighted variance in winter onset
    #formula variance for weighed sample, using frequency weighed unbiased estimator (~ bessel's correction)
    upper <- sum(weights*(w_on-mean_w_on)^2)
    lower <- sum(weights)-1
    if (lower >0){wsd <- sqrt(upper/lower)} else {wsd<-NA} #if the whole year is NA, sum(weights) is  ==> lower = -1 ==> wsd <-NA
    #in that case mean_w_on becomes NAN, but that causes no trouble. upper simply becomes 0
    
    #store mean_w_on and wsd in dataframe

    weighted[[i]][1:length(input[[1]][,1]),1]<-input[[1]][,1] #assigns names
    weighted[[i]][j,2]<-mean_w_on
    weighted[[i]][j,3]<-wsd
  }
}


```



####plotting new weighted sd's
plotting the results: mean, sd and weighted sd (all weighted). sd has to be capped at 50, CV at 0.5 due to a long right-tail

```{r plot_cv}
input<-NA
for (i in 1:length(weighted)){
  subtitle<-paste(x[i],"days below ", y[i], "°C") #will be needed in figures
 # hist(weighted[[i]]$sd,breaks=100, main = paste("histogram standard deviation run ",i))
  input<-weighted[[i]]
  input$sd[input$sd>50]<-50
  input<-input[!is.na(input$m),]
  a<-merge(input,locations,by=1)
  a<-a[!is.na(a$sd),]
  plot(a$lat~a$lon,bg=rgb(0,a$m,0,maxColorValue=372),col=NA,main="weighted mean day of winter   onset",sub=subtitle,pch=22,cex=0.8,xlab="",ylab="") #subtitle is recycled from earlier chunk (unweighted calcs)
  plot(a$lat~a$lon,bg=rgb(0,a$sd,0,maxColorValue=max(a$sd)),col=NA,main="Winter variability, weighed",sub= subtitle,pch=22,cex=0.8,xlab="",ylab="")
  points(x=(-170):(-121),y= rep(25,50), col = rgb (0, 0:50,0,maxColorValue = 50),pch=22,cex=0.8)
  text(-170,22,"sd=0")
  text(-120,22,"sd = 50")
  a$CV<-a$sd/a$m
 # hist(a$CV,breaks=100,main =paste ("histogram of CV run ",i))
  a$CV[a$CV>0.5]<-0.5
  plot(a$lat~a$lon,bg=rgb(0,a$CV,0,maxColorValue=max(a$CV)),col=NA,main="weighted mean/weighted sd of winter onset",sub=subtitle,pch=22,cex=0.8,xlab="",ylab="")
}
```

sd or cv, which one to choose?


```{r}
for (i in 1:4){
plot(weighted[[i]]$sd ~ weighted[[i]]$m)
}
```
Because standard deviation does not increase with the mean, there is no need to use coefficient of variation (sd/m)


####some extremely preliminary models:

```{r models}
a$alt[a$alt==-999.9]<-NA
a$sd[a$sd>100]<-100
a<-a[!is.na(a$alt),]

do_models<-function(variable,name){
M<-lm(variable~a$lat+a$lon+a$alt)

M2<-lm(variable~a$lat)
plot(variable~a$lat,main = paste(name," vs lat"))#,sub=paste("slope ~ ",round(coef(M2)[2]*10,4)," per 10°"))
abline(M2,col=2,lwd=2)

M3<-lm(variable~a$lon)
plot(variable~a$lon,main = paste(name," vs lon"))#,sub=paste("slope ~ ",round(coef(M3)[2]*100,4)," per 100°"))
abline(M3,col=2,lwd=2)

M4<-lm(variable~a$alt)
plot(variable~a$alt,main = paste(name, " vs altitude"))#,sub=paste("slope ~ ",round(coef(M4)[2]*1000,4), "per 1000m"))
abline(M4,col=2,lwd=2)
}

do_models(a$m,"mean")
do_models(a$sd,"sd")
#do_models(a$CV, "cv")
```


The mean winter onset decreases (becomes earlier) at higher latitudes and decreases with altitude (earlier high up). SD decreases very slighlty with latitude and altitude.

One would expect that SD is only high when amplitude (difference winter-summer) is low ,because then a small drop in temperature counts as winter event. Based on amplitude patterns(pink graphics, way up), one expects SD to increase towards equator, and to increase towards oceans. Instead, high SDs appear mostly in central US. Why?


```{r}

me<-merge(a,reg,by.x=1,by.y=6) #lat.y and lon.y are wrong
me$A[me$A>300]<-300 # a few really extreme points
me$alt[me$alt==-999.9]<-NA
me<-me[!is.na(me$alt),]
mod<-lm(me$sd~me$A)
plot(me$sd~me$A,pch=22,cex=0.3)
abline(mod,col=2,lwd=2)


me<-me[me$sd>1,]
Ml<-lm(log(me$sd)~me$A)
plot(log(me$sd)~me$A,pch=22,cex=0.5)
abline(Ml,col=2,lwd=2)

#So the variance occurs indeed only in low-amplitude regions (though the green graphics suggested otherwise)

me$logsd<-log(me$sd)

plot(me$logsd~me$lat.x)
abline(lm(me$logsd~me$lat.x),col=2,lwd=2)
#that is strange, Amplitude correlates with latitude, so why does sd not correlate the same way 


sub<-me[me$lon.x<0,]
hi<-sub[sub$sd>quantile(me$sd,0.98),]
plot(sub$lat.x~sub$lon.x,bg=rgb(sub$A,0,sub$A,maxColorValue = max(sub$A)),pch=22,col=NA,cex=1.5,xlim=c(-130,-50))
points(hi$lat.x~hi$lon.x,cex=1.5,col=4,lwd=2)
##aah!}

```

The amplitude does not correlate strongly with latitude between 20°N and 60°N, the continentality is more important. high sd (like, highest 2% of dataset) occur rarely at really high amplitude regions (>50°N and central US), but mostly at the west coast (which has low amplitude) and the mountain ranges east to it (medium amplitude)


```{r extract_n}
ns<-list(NA,NA,NA,NA)
for (i in 1:length(lis)){
  ns[[i]]<-data.frame(NA,NA)
  for (rows in 1:nrow(lis[[i]][[1]])){
    ns[[i]][rows,1]<-sum(!is.na(lis[[i]][[1]][rows,4:147]))
    ns[[i]][rows,2]<-lis[[i]][[1]][rows,1]
  }
  names(ns[[i]])<-c("n","ID")
}
s<-merge(ns[[1]],me,by.x=2,by.y=1)
plot(s$n,s$sd)
sub<-s[s$lon.x<0,]
hi<-sub[sub$sd>quantile(me$sd,0.98),]
hi2<-sub[sub$n<4,]

plot(sub$lat.x~sub$lon.x,bg=rgb(0,0,sub$n,maxColorValue = max(sub$n)),pch=22,col=NA,cex=0.8)

#plot(hi$lon.x,hi$lat.x,bg=rgb(0,0,0),pch=22,col=NA,cex=0.8,main = "location unusally high variation")
points(hi2$lat.x~hi2$lon.x,cex=1.5,col=2,lwd=2)

plot(s$lat.x~s$lon.x,bg=1,pch=22,col=NA,cex=0.8)
n3<-s[s$n==3,]
points(n3$lat.x~n3$lon.x,col=2)
```



