---
title: "read slopes"
author: "Jens Joschinski"
date: "March 15, 2018"
output: 
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(tidyverse)
library(textreadr)
library(drc)
library(sandwich)
library(lmtest)
library(plyr)
library(geosphere)
```


# General description  
## Aim  
Global change alters selection on phenology, due to rises in temperature means, variability and predictability. Species persistence will depend on how phenology evolves, which may encompass according changes in means, variability (bet-hedging) or plasticity.
We conducted a meta-analysis of arthropod diapause timing across species and climatic gradients. This script calculates means and slopes of day length reaction norms for 211 populations in 8 orders. The means and slopes of diapause will be correlated with mean, variability and predictability of winter onset, but that is in a different script.


### Overview  

The methods can be found in methods.md in the main folder. In short, we searched the Web of Science database for photoperiodic response curves of arthropods (search results: *.txt in folder 02studies/00raw) and selected 36 articles for analysis. For these articles we then extracted the x- and y-coordinates of the photoperiodic response curves from the figures (folder 02studies/01extracted_data/new). The whole procedure can be found in "populations.ods". We copied the data to the .csv file forslopes_new.csv, which will be used in this script. We now calculate the slope and other parameters of the photoperiodic response curve. 

### Specific description  

The data was generated with R version `r getRversion()`. Slope estimates derived with package drc, `r citation('drc')`.

One study (7 populations) reported the slope and midpoint from drc analyiss directly. The script concentrates on the remaining 35 studies. One of the studies (Kimura_geogr) was conducted on three species, these 3 species will be treated as independent. This makes 37 studies.

To estimate the midpoints and slopes we use the package drc. This analysis provides lower and upper bounds of photoperiodic induction, the slope, and the inflection point where 50 % of the individuals are induced (critical day length), so up to four parameters per slope will be estimated. We expected that all populations of one species have the same lower and upper limit, which would reduce the estimation to 2 parameters per slope, plus 2 global parameters per study. This expecteation does not always hold, however, so we also apply the alternative models with only one or no fixed global parameters. The following models are applied on each study (sorted by plausibility):

1. upper and lower parameter fixed at study mean (requires 2 df plus 2 df per population)
2. upper parameter fixed (requires 1 df plus 3 per population)
3. lower parameter fixed (requires 1 df plus 3 per population)
4. both limits vary (requires 4 df per population)

We choose the model with lowest AIC. If there are multiple models with an AIC difference less than 4, we use the most plausible model.

Detailed information on number of individuals per point estimate was rarely available. We used all data that was provided to weigh the individual points of the PRC. 

After fitting the curves, we do a bit of quality control and check whether some studies/populations need to be removed. 



### Script  

This script should not be executed completely, becaues it will cause some convergence failure. Also, all models need to be examined by hand (summary may show that some values are actually NA because errors occured without warning). It is rather a description of the methods than an actual executable script.


### load data  

```{r load_data}
data<-read.table("02studies/01extracted_data/forslopes_new.csv", header=T,sep=",")
length(unique(data$id)) #32 studies that can be used for slope calculation (kimura_geogr has 3 levels)
data<-unite(data,"popid",c("id","pop","genus","spec"),sep="-",remove=F)
length(unique(data$popid)) #182 populations (not 189 because lankinen)

#summary of #pops per region and level of detail for sample sizes

t<-data.frame(unique(data$popid))
t$reg<-NA
t$nMethod<-NA
#each unique pop has multiple rows in dataset (1 per day length)
#get this subset, take region and nMethod from first line (or use unique)
for ( i in 1:nrow(t)){ 
   t$reg[i]<-unique(data[data$popid==t[i,1],5])
   t$nMethod[i]<-unique(data[data$popid==t[i,1],13])
}
table(t$reg) #unfortunately, the information was converted to values
levels(data$region)
#Japan: 80+10 from the "usa,japan" = 90
#Europe: 62
#US: 12+11 from the "usa,japan" = 23
#China: 7
#this ignores 7 pops from Europe, where CDL and b were given directly in paper (not part of the analysis dataset)


table(t$nMethod)
levels(data$nmethod)

#accurate: 22
#global average: 146+7+3 =156
#pop level: 4

data[data$n2==1,15]<-100
#those with no described sample size (2 studies, 1 with no sample size given, 1 in japanese and sample size not found) get an arbitrary number of 100. No sample size is the same as global average anyway (means that individual-point weighing is not possible).

r<-data.frame(NA,NA,NA,NA,NA,NA,NA,NA,NA,NA)
names(r)<-c("name","b","b_rel","c","c_rel","d","d_rel","e","e_rel","nsl") #the results will be stored in a file with these columns

data$perc[data$perc<0]<-0
data$perc[data$perc>100]<-100
data$n2<-round(data$n2)
data$number<-(data$perc/100) * data$n2
data$number<-floor(data$number)
```

the following hcunk cannot be executed directly (some problem with optimizer). Change i, copy to console and run
```{r all_models}
i<-34
sub<-data[data$id==unique(data$id)[i],] 

#if pops have to be removed (do after fitting)
#sub<-sub[!sub$popid %in% unique(sub$popid)[c(3,4)],]

sub<-droplevels(sub)

#turn percentage into no. failures, no. successes (also round N)
#sub$n2<-round(sub$n2)
#sub$number<-round(sub$n2 * sub$perc/100)
#sub$number[sub$number>sub$n2]<-sub$n2[sub$number>sub$n2]
#sub$number[sub$number<0]<-0

aics <- c(NA,NA,NA,NA) #will store AIcs of up to four possible models 
estimates <- nrow(sub)
pops<-sub$pops_left[1]
if(length(pops)>1){print(paste("something went wrong in study ",sub$id[1],"\n"))}

#for those with ll.2. only
#other<-drm((number/n2)~dl2,curveid=popid,data=sub,fct=LL.2(),type="binomial",weights=n2)

# the four models

#model1: lower and upper limit fixed at study mean
res_df1 <- estimates - (2 + 2*pops)>3 #are there more than 3 df left?
drm1<- drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA),  #box constraints
           weights=n2,  
           pmodels=data.frame(popid,1,1,popid)) #this line distinguishes models 1-4 (fixing parameters at study mean)

aics[1]<-AIC(drm1)

#model2: upper parameter fixed
res_df2<-estimates - (1+ 3* pops)>3
drm2<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA), weights=n2, pmodels=data.frame(popid,popid,1,popid))  
aics[2]<-AIC(drm2)

#model3: lower parameter fixed
#res_df3 is same as res_df2
drm3<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA), weights=n2, pmodels=data.frame(popid,1,popid,popid))
aics[3] <- AIC(drm3)
  
#model 4: none fixed
res_df3 <- estimates - (4*pops)>3
drm4<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA),lowerl=c(NA,0,NA,NA), weights=n2)
 aics[4]<-AIC(drm4)
 

```

This chunk can run
```{r compare}
plot(drm1)
plot(drm2)
plot(drm3)
plot(drm4)
#are all models sensible (in terms of degrees of freedom)? 
print (paste("model 1: ", res_df1))
print (paste("model 2/3: ", res_df2))
print (paste("model 4: ", res_df3))
 
aics
aics[1]-min(aics)
aics[2]-min(aics)
aics[3]-min(aics)
aics[4]-min(aics)
```

change the value in choice and run this chunk
```{r choose}
choice <-3
final<-drm3
summary(final)
results<-list(final,choice)
```

run next 2 chunks, no need to edit
```{r save_plot}
  #possible values of "choice" and their meaning
  # NA "no model possible due to lack of df"
  #1 upper and lower parameter fixed at study mean
  #2 upper parameter fixed
  #3 lower parameter fixed
  #4 both limits vary
 if (results[[2]]==1){
    #plot study for visual validation
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep="")) 
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper and lower limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if (results[[2]]==2){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep="")) 
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if(results[[2]]==3){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final,xlab = "Day length", ylab = "percentage diapause", main = "lower limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if(results[[2]]==4){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper and lower limit estimated for each pop", log="", col=TRUE)
    dev.off()
  }else{
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "see notes", log="", col=TRUE)
    dev.off()
  }

```

```{r save_data}
    #write these into results table
    coffs<-coeftest(final,vcov = sandwich)[,1:2] #provides estimate and SE
    coffs<-as.data.frame(coffs)
    #save slope coefficients
    coffs$group<-substr(rownames(coffs),1,1)
    b<-coffs[coffs$group=="b",]
    c<-coffs[coffs$group=="c",]
    d<-coffs[coffs$group=="d",]
    e<-coffs[coffs$group=="e",]
    
    #if ll.2:
   # c<-data.frame(NA,NA,NA)
  #  names(c)<-c("Estimate" ,  "Std. Error", "group"     )
  #  d<-data.frame(NA,NA,NA)
  #  names(d)<-c("Estimate" ,  "Std. Error", "group"     )
    
    #need to bring these into sensible shape
    new_data<-data.frame(b,c,d,e)
    new_data<-new_data[,c(1,2,4,5,7,8,10,11)] #removes "group"
    names(new_data)<-c("b","b-se","c","c-se","d","d_se","e","e_se")
    new_data<-new_data[order(rownames(new_data)),]
   
    old_data<-ddply(sub, "popid", function(z) tail(z,1))
    results<-cbind(old_data,new_data)
  

write.table(results, "02studies/02output/slopes.txt",append=T,sep="\t",col.names=FALSE)
 print(paste(i,"|",results$id[1],"|",choice,sep=""))
```

i|study         |best   |chosen |impossible   |notes
1|chen          |1,3,4  |1      |
2|gomi          |LL.2   |LL.2   |all          |mod 1 would have missing data for 2 pops
3|gotoh         |3                            |consider removing ishikawa     
4|hashimoto     |1,3    |1                    |nagaoka and okayama removed            
5|ito           |2,4    |2                    |WA removed
6|kimura_geogr_1|3                            |consider removing OI
7|kimura_geogr_2|3,4    |3
8|kimura_geogr_3|1,3,4  |1      |            |evidence for 1 from 2 further pops
9|koveos        |4                            |thessaloniki1 and padua removed
10|kurota       |3    |3      |4   
11|lankinen     |3    |3      |             |evidence for 1/3 from 8 further pops; dietikon2 removed
12|lehmann      |NA    |NA      |             |removed
13|lumme        |2,4  |2
14|lushai       |NA  |NA                      |removed
15|murata       |NA    |NA                      |removed
16|musolin      |NA   |NA                     |removed
17|nechols      |LL.2 |LL.2                 |mod 1 would have NA for 1 pop, evidence from 7 further pops
18|paolucci     |1-4  |1                    |SCH looks odd
19|riihima      |2    |2      |             |removed oulo2
20|ryan         |1,2,3|1      |4
21|shimizu      |3,4  |3                    |SPR removed
22|shintani     |LL.2 |LL.2                 |mod1 would have NAs, evidence from 8 further pops
23|shroyer      |2    
24|so           |NA   |NA                   |removed
25|suwa         |1    |1      |             |removed all pops except 7,10,16,20; consider removing completely
26|tyukmaeva    |3,4  |3                    |removed lathi1,lathi3
27|ujiye        |NA    |NA      |NA         |removed
28|urbanski     |3    |3      |4              |1 and 2 without box constraints (bc fit is soo bad), 4 has NA for 1 pop
29|vaznunes     |3    |3      |4            |removed S1,S2,T2
30|wang         |1,3  |1
31|yoshida      |3    |3
32|kimura_evol  |NA   |NA                   |removed
33|takeda       |1,3,4|1                    |consider removing
34|noda         |3    |3      |               |removed dl<9; removed ishigaki


```{r quality_control}

results <- read.table("02studies/02output/slopes.txt",sep = "\t")
#first col is rownames. check if that is same as ID
#substr(as.character(results[,1]),start=3,stop =10000L)==as.character(results[,3])

names(results)<-c("row","set","popid","ID","PY","region","pops_left","genus","spec", "order", "pop","degN","degE","nmethod","perc","n2","dl2","number","nslop","inc2","b","bse","c","cse","d","dse","e","ese")

#removed 7 studies (21 pops) and 22 further pops

#182-21-22 = 139 pops left
#in: 25 studies (though one study has 3 specs)



#results[results$popid=="gotoh- Ishikawa-Tetranychus-urticae","use"]<-"n"


#checking distribution of standard errors (too small s.e. are aproblem because influence becomes infinite)
#b(slope)
results$invvar_b<-1/(results$bse^2)
plot(1:nrow(results),results$invvar_b, main = "influnece of data points on b",ylab="inverse variance [log]",log="y")
med<-median(results$invvar_b)
abline(h=med)
abline(h=med*10,col=2)
abline(h=med/10,col=2)
abline(h=med*5,col=4)
abline(h=med/5,col=4)
abline(h=med*50,col=1)
abline(h=med/50,col=1)
text(10,med*10,"median*10")
text(10,med*5,"median*5")
text(10,med*50,"median*50")
#some points are >100 times more influential than others
#
#points(x=which(results$n_slop<3),y=results$invvar_b[results$n_slop<3],pch=21,bg=2)

#points I consider  highest quality
points(x= which(results$ID=="urbanski"),y=results$invvar_b[which(results$ID=="urbanski")],col=2,pch=21,bg=2)
#points i consider lowest quality
points(x=which(results$ID=="suwa"),y=results$invvar_b[which(results$ID=="suwa")],col=4,pch=21,bg=4)

points(x=which(results$popid=="gotoh- Ishikawa-Tetranychus-urticae"), y=results$invvar_b[which(results$popid=="gotoh- Ishikawa-Tetranychus-urticae")], col=4,pch=21,bg=4)#no problem

points(x=which(results$popid=="kimura_geogr_1-OI-Drosophila-auraria"), y=results$invvar_b[which(results$popid=="kimura_geogr_1-OI-Drosophila-auraria")], col=4,pch=21,bg=4)#no problem

points(x=which(results$popid=="paolucci-SCH-Nasonia-vitripennis"), y=results$invvar_b[which(results$popid=="paolucci-SCH-Nasonia-vitripennis")], col=4,pch=21,bg=4)#no problem

points(x=which(results$ID == "takeda"), y=results$invvar_b[which(results$ID=="takeda")], col=4,pch=21,bg=4)#no problem

results$invvar_b[results$invvar_b>med*10]<-med*10
#15 inverse variance limited to 10*median



#same with e (CDL)
results$invvar_e<-1/(results$ese^2)
plot(1:144,results$invvar_e, main = "influnece of data points on e",ylab="inverse variance [log]",log="y")
med<-median(results$invvar_e)
abline(h=med)
abline(h=med*10,col=2)
abline(h=med/10,col=2)
abline(h=med*5,col=4)
abline(h=med/5,col=4)
abline(h=med*50,col=1)
abline(h=med/50,col=1)
text(10,med*10,"median*10")
text(10,med*5,"median*5")
text(10,med*50,"median*50")
#some points are >100 times more influential than others
#points i consider lowest quality
points(x=which(results$ID=="suwa"),y=results$invvar_e[which(results$ID=="suwa")],col=4,pch=21,bg=4)

points(x=which(results$popid=="gotoh- Ishikawa-Tetranychus-urticae"), y=results$invvar_e[which(results$popid=="gotoh- Ishikawa-Tetranychus-urticae")], col=4,pch=21,bg=4)#no problem

points(x=which(results$popid=="kimura_geogr_1-OI-Drosophila-auraria"), y=results$invvar_e[which(results$popid=="kimura_geogr_1-OI-Drosophila-auraria")], col=4,pch=21,bg=4)#no problem

points(x=which(results$popid=="paolucci-SCH-Nasonia-vitripennis"), y=results$invvar_e[which(results$popid=="paolucci-SCH-Nasonia-vitripennis")], col=4,pch=21,bg=4)#no problem

points(x=which(results$ID == "takeda"), y=results$invvar_e[which(results$ID=="takeda")], col=4,pch=21,bg=4)#no problem

points(x=which(results$n_slop<3),y=results$invvar_e[results$n_slop<3],pch=21,bg=2)


#points I consider  highest quality
points(x= which(results$ID=="urbanski"),y=results$invvar_e[which(results$ID=="urbanski")],col=2,pch=21,bg=2)
#points i consider lowest quality
points(x=which(results$ID=="suwa"),y=results$invvar_e[which(results$ID=="suwa")],col=4,pch=21,bg=4)
results$invvar_e[results$invvar_e>med*10]<-med*10
#27 inverse variances reset to median*10

write.table(results,"02studies/02output/slopes_clean.txt",sep = "\t",row.names=F)
```




Now the slope and cdl estimates are done. After combining with climate data it turned out that the CDL data is quite far away (2 months) from the mean winter onset. This chunk explains the calculation of the julian date that is most consistent with onset of diapause
```{r}

results <- read.table("02studies/02output/slopes.txt",sep = "\t")
#first col is rownames. check if that is same as ID
substr(as.character(results[,1]),start=3,stop =10000L)==as.character(results[,3])

names(results)<-c("row","set","popid","ID","PY","region","pops_left","genus","spec", "order", "pop","degN","degE","nmethod","perc","n2","dl2","number","b","bse","c","cse","d","dse","e","dse")
results<-results[-39,] #was empty

results<-results[order(results$degN),]

url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional separators
  widths=c(11, 9, 10, 7,2,35)
)
reslist<-read.table("01climate_data/03output/results.txt",na.string = c("NA","-9999","-999.9"))
names(reslist)<-c("ID","meanwinter","sd_winter","nyears","p","A","phi","c","beta")
climate<-merge(locations,reslist,by=1)
rm(locations)
rm(reslist)
names(climate)<-c("ID","lat","lon","alt","no_idea","name","meanwinter","sd_winter", "nyears","p","A","phi","c","beta")
climate<-climate[climate$lat<=70,]
plot(climate$meanwinter~climate$lat,pch=22,cex=0.1, main ="Mean winter onset vs. latitude")
M<-lm(climate$meanwinter~climate$lat)


#1. Let us assume that winter onset is on day 330 (an arbitrary day). Then one can calculate the "optimal" CDL response given latitude and day 330:
plot(results$e~results$degN,main = "CDL vs latitude, red= expectation if winter onset =180")
dl<-daylength(results$degN,330)
points(dl~results$degN,col=2)
#a model of the form (expected ~ observed) should give a quite good fit, because there is a good (negative) correlation. But this is not quite what I want, as they should not only correlate but be more or less identical. An alternative model of form (expted~observed - intercept) should give, on the other hand, a really bad fit

plot(dl~results$e,main = "expected(for day=180) vs observed.",sub="setting intercept to 0")
M1<-lm(dl~results$e)
M2<-lm(dl~results$e-1)
abline(M1,col=1,lty=2)
abline(M2,col=2)

#bringing that back to the CDL~latitude plot:
plot(results$e~results$degN,main = "CDL vs latitude", sub="red= expectation if winter onset =180\nblue = model predictions without intercept",ylim=c(4,22),xlab="")
points(dl~results$degN,col=2)
#points(predict(M)~results$degN,col="green")
points(predict(M2)~results$degN,col=4)
#this model is indeed a bad fit to the data. 

#the same can now be done for all 365 days, and then the model with the highest log-likelihood should be selected


```

```{r}
logliks<-rep(NA,365)
for ( i in 1:365){
  dl<-daylength(results$degN,i)
  M<-lm(dl~results$e-1)
  logliks[i]<-logLik(M)
  #abline(M)
}
plot(logliks,type="l", main ="logLikelihood profile for different dates",xlab = "Julian date", ylab = "LogLikelihood")
points(logliks,pch=22,cex=0.3,bg=1)#160-183
order(logliks,decreasing=T)[1:10]
#135, 211,  212,134,136
plot(results$e~results$degN,main = "CDL vs latitude", sub="red= expectation if winter onset =135\nblue = 211\ngreen =180",ylim=c(4,22),xlab="")
dlbest<-daylength(results$degN,144)
dlsecond<-daylength(results$degN,202)
dlbad=daylength(results$degN,180)

points(dlbest~results$degN,col=2)
points(dlsecond~results$degN,col=4)
points(dlbad~results$degN,col="green")
#best and second best are equal
M<-lm(dlbest~results$e-1)
M2<-lm(dlsecond~results$e-1)
```
The best estimates by this procedure are day 135 (may 15th) and day 211 (jul 30th), these estimates are exactly equal, as they are both 38 days away from summer solstice. 

checking whether calculation is correct with https://www.esrl.noaa.gov/gmd/grad/solcalc/
```{r}
plot(results$e~results$degN,main = "CDL vs latitude", sub="red= expectation if winter onset =30jul\nblue = 1.dec",ylim=c(4,22),xlab="")
dlbest<-daylength(results$degN,135)
dlmeteorolog<-daylength(results$degN,305)
points(dlsecond~results$degN,col=2)
points(dlmeteorolog~results$degN,col=4)
Nvec<-10*(1:7)
dlvec<-c(18,18,18,19,19,20,22)+c(23,38,55,16,46,35,58)/60 - (c(5,5,5,4,4,3,1)+c(49,35,18,56,26,36,05)/60) #taken from noaa calculator for Nvec °N, 0°E
dlvec2<-c(17,17,17,16,16,15)+c(37,19,0,35,1,3)/60 - 
  (c(6,6,6,7,7,8)+c(1,19,38,2,37,35)/60)
dlvec2<-c(dlvec2,0)

points(dlvec~Nvec,col="green")
points(dlvec2~Nvec,col="black",pch=22,bg=1) # NOAA uses "apparent sunrise", could be civil sunrise/sunset
```

