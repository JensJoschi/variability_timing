---
title: "read slopes"
author: "Jens Joschinski"
date: "March 15, 2018"
output: 
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(plyr)
library(tidyverse)
library(textreadr)
library(drc)
library(sandwich)
library(lmtest)
library(geosphere)
library(metafor)
```


# General description  
## Aim  
Global change alters selection on phenology, due to rises in temperature means, variability and predictability. Species persistence will depend on how phenology evolves, which may encompass according changes in means, variability (bet-hedging) or plasticity.
We conducted a meta-analysis of arthropod diapause timing across species and climatic gradients. This script calculates means and slopes of day length reaction norms for 146 populations in 8 orders. The means and slopes of diapause will be correlated with mean, variability and predictability of winter onset, but that is in a different script.


### Overview  

The methods can be found in methods.md in the main folder. In short, we searched the Web of Science database for photoperiodic response curves of arthropods (search results: *.txt in folder 02studies/00raw) and selected 36 articles for analysis. For these articles we then extracted the x- and y-coordinates of the photoperiodic response curves from the figures (folder 02studies/01extracted_data/new). The whole procedure can be found in "populations.ods". We copied the data to the .csv file forslopes_new.csv, which will be used in this script. We now calculate the slope and other parameters of the photoperiodic response curve. 

### Specific description  

The data was generated with R version `r getRversion()`. Slope estimates derived with package drc, `r citation('drc')`.

One study (7 populations) reported the slope and midpoint from drc analyiss directly. The script concentrates on the remaining 35 studies. One of the studies (Kimura_geogr) was conducted on three species, these 3 species will be treated as independent. This makes 37 studies.

To estimate the midpoints and slopes we use the package drc. This analysis provides lower and upper bounds of photoperiodic induction, the slope, and the inflection point where 50 % of the individuals are induced (critical day length), so up to four parameters per slope will be estimated. We expected that all populations of one species have the same lower and upper limit, which would reduce the estimation to 2 parameters per slope, plus 2 global parameters per study. This expecteation does not always hold, however, so we also apply the alternative models with only one or no fixed global parameters. The following models are applied on each study (sorted by plausibility):

1. upper and lower parameter fixed at study mean (requires 2 df plus 2 df per population)
2. upper parameter fixed (requires 1 df plus 3 per population)
3. lower parameter fixed (requires 1 df plus 3 per population)
4. both limits vary (requires 4 df per population)

We choose the model with lowest AIC. If there are multiple models with an AIC difference less than 4, we use the most plausible model.

Detailed information on number of individuals per point estimate was rarely available. We used all data that was provided to weigh the individual points of the PRC. 

After fitting the curves, we do a bit of quality control and check whether some studies/populations need to be removed. 



### Script  

This script should not be executed completely, becaues it will cause some convergence failure. Also, all models need to be examined by hand (summary may show that some values are actually NA because errors occured without warning). It is rather a description of the methods than an actual executable script.


### load data  

```{r load_data}
data<-read.table("02studies/01extracted_data/forslopes_new.csv", header=T,sep=",")
length(unique(data$id)) #32 studies that can be used for slope calculation (kimura_geogr has 3 levels)
data<-unite(data,"popid",c("id","pop","genus","spec"),sep="-",remove=F)
length(unique(data$popid)) #182 populations (not 189 because lankinen 2013 is not in here)

#summary of #pops per region and level of detail for sample sizes

t<-data.frame(unique(data$popid))
t$reg<-NA
t$nMethod<-NA
#each unique pop has multiple rows in dataset (1 per day length)
#get this subset, take region and nMethod from first line (or use unique)
for ( i in 1:nrow(t)){ 
   t$reg[i]<-unique(data[data$popid==t[i,1],5])
   t$nMethod[i]<-unique(data[data$popid==t[i,1],13])
}
table(t$reg) #unfortunately, the information was converted to values
levels(data$region)
#Japan: 80+10 from the "usa,japan" = 90
#Europe: 62
#US: 12+11 from the "usa,japan" = 23
#China: 7
#this ignores 7 pops from Europe, where CDL and b were given directly in paper (not part of the analysis dataset)


table(t$nMethod)
levels(data$nmethod)

#accurate: 22
#global average: 146+7+3 =156
#pop level: 4

data[data$n2==1,15]<-100
#those with no described sample size (2 studies, 1 with no sample size given, 1 in japanese and sample size not found) get an arbitrary number of 100. No sample size is the same as global average anyway (means that individual-point weighing is not possible).

r<-data.frame(NA,NA,NA,NA,NA,NA,NA,NA,NA,NA)
names(r)<-c("name","b","b_rel","c","c_rel","d","d_rel","e","e_rel","nsl") #the results will be stored in a file with these columns

data$perc[data$perc<0]<-0
data$perc[data$perc>100]<-100
data$n2<-round(data$n2)
data$number<-(data$perc/100) * data$n2
data$number<-floor(data$number)
```
Now the drc analysis is performed individually for each study. The following chunk cannot be executed directly (some problem with optimizer). Change i, copy the whole chunk to console and run.

```{r all_models}
i<-34
sub<-data[data$id==unique(data$id)[i],] 

#if pops have to be removed (do after fitting)
#sub<-sub[!sub$popid %in% unique(sub$popid)[c(3,4)],]

sub<-droplevels(sub)


aics <- c(NA,NA,NA,NA) #will store AIcs of up to four possible models 
estimates <- nrow(sub)
pops<-sub$pops_left[1]

# some studies require a ll.2, see methods. in this case:
#other<-drm((number/n2)~dl2,curveid=popid,data=sub,fct=LL.2(),type="binomial",weights=n2)

# the four models

#model1: lower and upper limit fixed at study mean
res_df1 <- estimates - (2 + 2*pops)>3 #are there more than 3 df left?
drm1<- drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA),  #box constraints
           weights=n2,  
           pmodels=data.frame(popid,1,1,popid)) #this line distinguishes models 1-4 (fixing parameters at study mean)

aics[1]<-AIC(drm1)

#model2: upper parameter fixed
res_df2<-estimates - (1+ 3* pops)>3
drm2<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA), weights=n2, pmodels=data.frame(popid,popid,1,popid))  
aics[2]<-AIC(drm2)

#model3: lower parameter fixed
#res_df3 is same as res_df2
drm3<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA), lowerl=c(NA,0,NA,NA), weights=n2, pmodels=data.frame(popid,1,popid,popid))
aics[3] <- AIC(drm3)
  
#model 4: none fixed
res_df3 <- estimates - (4*pops)>3
drm4<-drm((number/n2)~dl2, curveid=popid, data=sub, fct=LL.4(), type="binomial", upperl=c(NA,NA,1,NA),lowerl=c(NA,0,NA,NA), weights=n2)
 aics[4]<-AIC(drm4)
 

```

The preceding chunk ran the four possible models, and these will be now compared by AIC. 
(This chunk can run, no need to copy into console)
```{r compare}
plot(drm1)
plot(drm2)
plot(drm3)
plot(drm4)
#are all models sensible (in terms of degrees of freedom)? 
print (paste("model 1: ", res_df1))
print (paste("model 2/3: ", res_df2))
print (paste("model 4: ", res_df3))
 
aics
aics[1]-min(aics)
aics[2]-min(aics)
aics[3]-min(aics)
aics[4]-min(aics)
```


After choosing one of the models, change the value in "choice" and "final" and run this chunk
```{r choose}
choice <-3
final<-drm3
summary(final)
results<-list(final,choice)
```

Now that the model is selected for each study, a simple plot of it will be saved (for later checking, not high-quality plots as in papers). This chunk can run, no need to copy to console.

```{r save_plot}
  #possible values of "choice" and their meaning
  # NA "no model possible due to lack of df"
  #1 upper and lower parameter fixed at study mean
  #2 upper parameter fixed
  #3 lower parameter fixed
  #4 both limits vary
 if (results[[2]]==1){
    #plot study for visual validation
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep="")) 
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper and lower limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if (results[[2]]==2){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep="")) 
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if(results[[2]]==3){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final,xlab = "Day length", ylab = "percentage diapause", main = "lower limit fixed at global mean", log="", col=TRUE)
    dev.off()
  }else if(results[[2]]==4){
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "upper and lower limit estimated for each pop", log="", col=TRUE)
    dev.off()
  }else{
    png(paste("02studies/01extracted_data/",i,"-",unique(data$id)[i],".png",sep=""))
    plot(final, xlab = "Day length", ylab = "percentage diapause", main = "see notes", log="", col=TRUE)
    dev.off()
  }

```

Lastly, the estimates of b and CDL need to be extracted and saved, along with their standard errors. This chunk runs, except for ll.2 in which lines 220-225 have to be used.

```{r save_data}
    #write these into results table
    coffs<-coeftest(final,vcov = sandwich)[,1:2] #provides estimate and SE
    coffs<-as.data.frame(coffs)
    #save slope coefficients
    coffs$group<-substr(rownames(coffs),1,1)
    b<-coffs[coffs$group=="b",] #b = slope of the logit-curve
    c<-coffs[coffs$group=="c",] #c = lower limit of the curve
    d<-coffs[coffs$group=="d",] #d = upper limit
    e<-coffs[coffs$group=="e",] #e = CDL, midpoint, inflection point of the curve
    
    #if ll.2:
  # c<-data.frame(NA,NA,NA)
  #  names(c)<-c("Estimate" ,  "Std. Error", "group"     )
  #  d<-data.frame(NA,NA,NA)
  #  names(d)<-c("Estimate" ,  "Std. Error", "group"     )
    
    #need to bring these into sensible shape
    new_data<-data.frame(b,c,d,e)
    new_data<-new_data[,c(1,2,4,5,7,8,10,11)] #removes "group"
    names(new_data)<-c("b","b-se","c","c-se","d","d_se","e","e_se")
    new_data<-new_data[order(rownames(new_data)),]
   
    old_data<-ddply(sub, "popid", function(z) tail(z,1))
    results<-cbind(old_data,new_data)
  

write.table(results, "02studies/02output/slopes.txt",append=T,sep="\t",col.names=FALSE)
 print(paste(i,"|",results$id[1],"|",choice,sep=""))
```
What follows is the summary of the analyses:
The numbers correspond to models drm1, drm2, drm3 and drm4

i|study         |best   |chosen |impossible   |notes
1|chen          |1,3,4  |1      |
2|gomi          |LL.2   |LL.2   |all          |mod 1 would have missing data for 2 pops
3|gotoh         |3                            |consider removing ishikawa     
4|hashimoto     |1,3    |1                    |nagaoka and okayama removed            
5|ito           |2,4    |2                    |WA removed
6|kimura_geogr_1|3                            |consider removing OI
7|kimura_geogr_2|3,4    |3
8|kimura_geogr_3|1,3,4  |1      |            |evidence for 1 from 2 further pops
9|koveos        |4                            |thessaloniki1 and padua removed
10|kurota       |3    |3      |4   
11|lankinen     |3    |3      |             |evidence for 1/3 from 8 further pops; dietikon2 removed
12|lehmann      |NA    |NA      |             |removed
13|lumme        |2,4  |2
14|lushai       |NA  |NA                      |removed
15|murata       |NA    |NA                      |removed
16|musolin      |NA   |NA                     |removed
17|nechols      |LL.2 |LL.2                 |mod 1 would have NA for 1 pop, evidence from 7 further pops
18|paolucci     |1-4  |1                    |SCH looks odd
19|riihima      |2    |2      |             |removed oulo2
20|ryan         |1,2,3|1      |4
21|shimizu      |3,4  |3                    |SPR removed
22|shintani     |LL.2 |LL.2                 |mod1 would have NAs, evidence from 8 further pops
23|shroyer      |2    
24|so           |NA   |NA                   |removed
25|suwa         |1    |1      |             |removed all pops except 7,10,16,20; consider removing completely
26|tyukmaeva    |3,4  |3                    |removed lathi1,lathi3
27|ujiye        |NA    |NA      |NA         |removed
28|urbanski     |3    |3      |4              |1 and 2 without box constraints (bc fit is soo bad), 4 has NA for 1 pop
29|vaznunes     |3    |3      |4            |removed S1,S2,T2
30|wang         |1,3  |1
31|yoshida      |3    |3
32|kimura_evol  |NA   |NA                   |removed
33|takeda       |1,3,4|1                    |consider removing
34|noda         |3    |3      |             |removed dl<9; removed ishigaki


All models were done semi-automatically, now we need to check what the data looks like.

```{r data_summary}
results <- read.table("02studies/02output/slopes.txt",sep = "\t") #reload results
#first col is rownames. check if that is same as ID
#substr(as.character(results[,1]),start=3,stop =10000L)==as.character(results[,3]) #all true

names(results)<-c("row","set","popid","ID","PY","region","pops_left","genus","spec", "order", "pop","degN","degE","nmethod","perc","n2","dl2","number","nslop","inc2","b","bse","c","cse","d","dse","e","ese") #cols "nmethod" to "inc2" will not be used anymore
#table(results$popid)
#kimura_geogr_1 accidentally added twice
results<-results[-c(21:25),]
results<-droplevels(results)


#removed 7 studies (21 pops) and 22 further pops
#182-21-22 = 139 pops left
#in: 25 studies (though one study has 3 specs)
```


Several populations are on the brink of being overfitted, but there is no simple test for that. Populations with very vew estimates (day lengths) get standard errors close to zero, even though they are least reliable. Weighing them by the inverse of their variance (1/s.e.^2) would be problematic, because they get nearly infinite weight despite being least reliable.
Demonstration of this issue:

```{r funnel_plots}
#this model is only for demonstration purposes, actual analysis will run in different script
results$col<-hsv((as.numeric(results$order)-1)/8,1,1) # 8 different colors, 1 per invertebrate order
rmamod_rednested<-rma.mv(yi = e ~ degN, V = 1, random = ~1|order/genus/ID,data = results) # latitude as fixed factor, no weighing

svg("funnel_ese.svg",pointsize=12,width=10)
plot(x=resid(rmamod_rednested), y=results$ese, ylim=c(1,0), xlab = "Model residuals", ylab = "Standard error", bty="n",pch = 21, bg= results$col, xlim=c(-3.5,3.5), yaxs ="i", cex.axis=1.5, cex.lab = 1.5)
polygon(c(-5,5,5,-5),c(2,2,-2,-2),col="darkgrey")
polygon(c(-1.96,1.96,0,-1.96),c(1,1,0,1),col=0,lty=0)
lines(x=c(0,1.96),y=c(0,1),lty=3)
lines(x=c(0,-1.96),y=c(0,1),lty=3)
abline(v=0,lty=1)
points(x=resid(rmamod_rednested), y=results$ese,pch = 21, bg= results$col,xpd=T)
arrows(-0.3972738,0.9,-0.3972738,1,length=0.1) #goes to 8.259408
dev.off()


#same for b
rmamod_rednested<-rma.mv(yi = b~1, V = 1, random = ~1|order/genus/ID,data = results)

svg("funnel_bse.svg",pointsize=12,width=10)
plot(x=resid(rmamod_rednested), y=results$bse, ylim=c(80,0), xlab = "Model residuals", ylab = "Standard error", bty="n",pch = 21, bg= results$col,xlim=c(-170,170),yaxs="i", cex.axis=1.5, cex.lab = 1.5)
polygon(c(-200,200,200,-200),c(100,100,-20,-20),col="darkgrey")
polygon(c(-1.96*100,1.96*100,0,-1.96*100),c(100,100,0,100),col=0,lty=0)
lines(x=c(0,1.96*100),y=c(0,100),lty=3)
lines(x=c(0,-1.96*100),y=c(0,100),lty=3)
abline(v=0,lty=1)
points(x=resid(rmamod_rednested), y=results$bse,pch = 21, bg= results$col)
axis(1,cex.axis=1.5)
axis(2,cex.axis=1.5)
arrows(-46.6847,70,-46.6847,80,length=0.1) #goes to 152.6679
dev.off()

```

These funnel plots show that points with high standard errors spread less around expectation than points with low standard errors (i.e. the funnel goes wrong way). especially for slope estimates with s.e. between 0 and 20. For higher s.e. it becomes better (spread proportional to s.e.)

Lets try a different way of weighing, based on the no. of points on the sloped part. We count all points on sloped part plus first point on the upper and lower limit. These are the points that mostly define shape of drc. It was manually done by counting the points on the fittted curves.

```{r n_points}
results$npoints <- c(4,4,5, 5,4,4,3,3,4, 4,7,4,7, 5,4,4,4, 4,4,5, 4,4,3,5,4, 4,4,4,3, 6,5,6, 4,4,3,5,4, 4,4,4, 4,6,5,3,4,3,4,4, 5,5,5, 4,4,3, 4,5,7,4, 3,5,4, 10,7,4, 5,4,4, 4,3,4,3, 4,4,4, 4,4,4,4, 5,5,5,4,4,4,4,3,4,5,4,4,3,4,3,4,5,5, 9,10,6,8,9,6,8,7,10,5,9,10,6,6,8,10,9,8,6,10,10, 4,4,6,4,5,4, 5,4,4,4, 5,5,4, 4,6,4, 5,4,4,4,4,5)

rmamod_rednested<-rma.mv(yi = e ~ degN, V = 1, random = ~1|order/genus/ID,data = results)
svg("funnel_n_e.svg",pointsize=12,width=10) #this is the same as before, only with a differnet y axis
plot(x=resid(rmamod_rednested), y=results$npoints ,ylim=c(3,10), xlab = "Model residuals", ylab = "Sample size", bty="n",pch = 21, bg= results$col,xlim=c(-3,3),yaxs="i", cex.axis=1.5, cex.lab = 1.5,xpd=T)
abline(v=0,lty=1)
dev.off()

rmamod_rednested<-rma.mv(yi = b ~ 1, V = 1, random = ~1|order/genus/ID,data = results)
svg("funnel_n_b.svg",pointsize=12,width=10)
plot(x=resid(rmamod_rednested), y=results$npoints ,ylim=c(3,10), xlab = "Model residuals", ylab = "Sample size", bty="n",pch = 21, bg= results$col,xlim=c(-150,150),yaxs="i", cex.axis=1.5, cex.lab = 1.5,xpd=T)
abline(v=0,lty=1)
dev.off()
```

These funnel plots look much better, because they are in the right direction now. The shading "funnel" is missing in these plots, because it cannot be calculated with sample sizes only. See also ?funnel. 

One publication is misssing so far, that is lankinen 2013. They gave b and CDL estimates directly so there was no need to extract data with a drc analysis. The paper does not directly mention standard error of the estimates, but this is not neccessary anyway with a sample-size based approach. What would be needed though is the snumber of points that contribute to the slope estimation. The only mention in the study is that there are 7 day length treatments, which seems to include multiple points at 0 or 100 %. I take the median sample size (4) of all data, so that the dataset has an average weight. The closely related study by tyukmaeva has same methods and gets 3-5 (median 4) day lengths per population, so that seems to fit.

```{r lankinen}
lank <- data.frame(
  row = c("b:lankinen_13-Pelkosenniemi-Drosophila-montana", "b:lankinen_13-Oulanka-Drosophila-montana", "b:lankinen_13-Kemi-Drosophila-montana", "b:lankinen_13-Pudasjarvi-Drosophila-montana", "b:lankinen_13-Paltamo-Drosophila-montana", "b:lankinen_13-ivaskila-Drosophila-montana","b:lankinen_13-lahti-Drosophila-montana"),
  set=rep("byhand",7),
  popid=c("b:lankinen_13-Pelkosenniemi-Drosophila-montana", "b:lankinen_13-Oulanka-Drosophila-montana", "b:lankinen_13-Kemi-Drosophila-montana", "b:lankinen_13-Pudasjarvi-Drosophila-montana", "b:lankinen_13-Paltamo-Drosophila-montana", "b:lankinen_13-ivaskila-Drosophila-montana","b:lankinen_13-lahti-Drosophila-montana"),
  ID = rep("lankinen_13",7), PY = rep(2013,7), region = rep ("Europe",7), pops_left= rep(7,7), genus = rep("Drosophila",7), spec = rep("montana",7), order = rep("Diptera",7), 
  pop = c("Pelkosenniemi", "Oulanka", "Kemi", "Pudasjarvi", "Paltamo", "ivaskila","lahti"), 
  degN =c(67.1, 66.4, 65.7, 65.4, 64.3, 62.2, 61.1),
  degE = c(27.3, 29.2, 24.7, 27.0, 27.9, 25.7, 25.7),
  nmethod = rep(NA,7), perc = rep(NA,7), n2 = rep(NA,7), dl2= rep(NA,7), number= rep(NA,7),nslop=rep(NA,7),inc2=rep(NA,7),
  b=c(35.59, 26.40, 39.38, 27.91, 43.03, 36.72, 45.38),
  bse = rep(NA,7),# c(11.61, 7.04, 19.64, 7.25, 13.84, 14.22, 12.81),
  c = rep(0,7),cse = rep(NA,7),d=rep(1,7),dse = rep(NA,7),
  e = c(19.22, 19.38, 18.63, 19.46, 18.78, 18.32, 17.70),
  ese = rep(NA,7), #c(0.46,0.72,1.13, 1.00, 0.42, 0.75, 0.34)),
  col = rep( "#FFBF00",7),
  npoints = rep(4,7))

results<-rbind(results,lank)
                   
write.table(results,"02studies/02output/slopes_clean.txt",sep = "\t",row.names=F)
```


```{r}
results<-read.table("02studies/02output/slopes_clean.txt",header=T)
d<-c(0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8)
within<- sum(d*(1-d))
between<-sd(d)/((sqrt(length(d)))^2)
results$within<-NA
results$between<-NA

##look after c,d = NA [set to 0,1]
results$c[is.na(results$c)]<-0
results$d[is.na(results$d)]<-1 #ll.2 was performed in these cases, and c and d are defined to be 0 and 1

logcurve<-function(x,b,c,d,e){(d-c)/(1+exp(-b*(x-e)))+c}

for(i in 1:nrow(results)){
  x<-seq(results$e[i]-4, results$e[i]+4, length.out =1000)
  y<-logcurve(x,results$b[i],results$c[i],results$d[i],results$e[i])
  results$within[i]<- sum(y*(1-y))
  results$between[i]<-(sd(y)/(sqrt(length(y))))^2
}

write.table(results,"02studies/02output/slopes_clean.txt",sep = "\t",row.names=F)
```

