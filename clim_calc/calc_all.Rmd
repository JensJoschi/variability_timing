---
title: "Calculation of climate means and predictability"
author: "Jens Joschinski"
date: "September 17, 2018"
output: 
    md_document:
        variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache=TRUE)
library(RCurl)
library(readr)
library(data.table)
library(textreadr)
library(tidyr)
library(dplyr)
library(stringr)
library(magrittr)
library(imputeTS)
```

# General description  
## Project aim  
To understand the evolvability of phenological strategies such as plasticity and bet-hedging, we correlate variance components and means of reaction norms with climate parameters. 


## Script overview  
The script calculates mean winter onset and winter predictability for all multiple climate stations in the northern hemisphere. The data will be combined with reaction norms of various arthropod species in separate scripts. 
The script first has a general part: it uses the daily minimum and maximum temeratures for all months, years and climate stations all over the world from the GHCN dataste (~ 12million X 31 days).These were pre-processed with PERL and saved as two txt files. The two datasets are now opened, cleaned, and the mean of min and max is calculated for each day. The resulting workspace with one matrix of 11 mil x 31days is saved as R workspace. Then the data is transposed to a matrix of 934k station - years * 365 days, and saved as .txt file.  

The second part of the script calculates mean and predictability of winter onset. This calculation depends on the definition of winter, and for a sensitivity analysis a larger range of definitions is used in a loop. For each definition the script takes about 30 minutes to complete. The resulting data is saved as .txt file.

The last part calculates predictability as colour of noise, i.e. based on autocorrelations in the temperature data. This part has to run only once.


The data was generated with R version `r getRversion()`. It uses the GHCN-daily dataset by NOAA:
```{r, echo =F}
text<- read_document("ghcnd-version.txt")
print (text[1])
```


# Script  
## data preparation  

### calculate mean temperature  

The following chunk opens the two datasets that were extracted from the GHCN-D database (min and max temperature). Those rows that occur in both datasets are used to calculte mean temperature. Everything else is discarded 

```{r join}
converted<-fread("clim_read\\converted_max.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F) #12,616,909 rows
length(unique(converted$V1)) #34,145                                                                  
converted2<-fread("clim_read\\converted_min.txt", sep ="|", na.strings=c("NA","-9999"),verbose=F) #12,658,376
length(unique(converted2$V1)) #34,052 


converted<-unite_(converted,"new_id",c("V1","V2","V3"),remove=FALSE)
converted2<-unite_(converted2,"new_id",c("V1","V2","V3"),remove=FALSE)

names(converted)<-c("new_id","ID","year","month",1:31)
names(converted2)<-c("new_id","ID","year","month",1:31)

n1<-unique(converted$new_id)
n2<-unique(converted2$new_id)
both <- n1[n1 %in% n2] #has set of rows that occur in both dataset
rm(n1)
rm(n2)

converted2<-converted2[converted2$new_id %in% both]
converted<-converted[converted$new_id %in% both]
nrow(converted)#12,543,632
rm(both)
#we are now down to 12.5 million rows in each dataset

converted<-arrange(converted, new_id)
converted2<-arrange(converted2, new_id)

join<-converted
join[,5:35]<-(converted[,5:35]+converted2[,5:35])/2
plot(join[500:550,10],col=2,pch=21,bg=2)
segments(1:51,converted[500:550,10],1:51,converted2[500:550,10])
#seems to work
rm(converted)
rm(converted2)


```


### clean data  
The following chunk removes:   
- data from southern hemisphere
- years with <50% data present
- stations with <3 years of data
```{r clean_data}
#remove all stations from southern hemisphere

#url<-"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
url<-"ghcnd-stations.txt" #use local copy instead of url
#this dataset is fixed-width delimited, requiring a few additional steps
locations<-read.fwf(
  file=url
  ,sep="!",na.strings=c("NA","-999.9"), #sep = ! because ! does not exist in dataset - > dataset is fixed-width and should have no additional separators
  widths=c(11, 9, 10, 7,2,35)
)

badlist <- locations[locations[,2]<0,1]  #latitudes smaller 0Â°N
rm(locations)
rm(url)

join<-join[!(join$ID %in% badlist),]
nrow(join) #11,755,351
rm(badlist)


#remove all years with >50% missing
join<-unite_(join,"stationyears",c("ID","year"),remove=FALSE)
join$days<-rowSums(!is.na(join[,6:ncol(join)]))
x<-group_by(join,stationyears)
y<-summarise(x,sum = sum(days))
#nrow(y) #1,058,817station-years

#nrow(y[y$n>364,])/nrow(y)#42.8% off all station-years are complete (no NA)
#nrow(y[y$n>350,])/nrow(y)#66.3% have more than 350 days
#nrow(y[y$n>300,])/nrow(y)#78.7% have more than 300 days
nrow(y[y$sum<182,])/nrow(y)#11.3% are less than half complete
inlist<-y$stationyears[y$sum>182] #937,608 entries
join<-join[join$stationyears %in% inlist,]
nrow(join)# 11,026,854
rm(inlist)

#remove all stations with <3 ys of data
x<-count(join,ID,year)
x$ID<-factor(x$ID)
ny<-count(x,ID)
#mean(ny$nn)*n_distinct(x$ID) #mean no years * total number stations = number rows in dataset
#n_distinct(join$stationyears)
include<-ny$ID[ny$nn>2] 
length(include) #26,804 stations can be included
length(ny$nn) #29,095 in total

rm(x)
rm(y)
rm(ny)
join<-join[join$ID %in% include,]
rm(include)
nrow(join)#10,991,727
length(unique(join$ID)) #26,804
```
### saving  

a bit of further cleaning, and saving the workspace
```{r save}
join<-join[,-(1:2)]
names(join)<-c("ID","year","month",1:31,"days")
mymonths <- c("07Jan","08Feb","09Mar",
              "10Apr","11May","12Jun",
              "01Jul","02Aug","03Sep",
              "04Oct","05Nov","06Dec")
join$month<-mymonths[join$month]  #when it is sorted alphabetically it now starts in july, and ends in jun
converted<-join
rm(join)
save.image(file = "converted.RData") 
#row 1         ID year month   1     2     3   4     5     6   7   8   9    10  11  12  13  14  15
#1 ACW00011604 1949 07Jan 253 258.5 252.5 258 255.5 255.5 253 242 247 230.5 228 236 214 225 228
#     16  17    18  19    20    21    22  23  24    25  26    27    28  29  30    31
#1 244.5 239 241.5 233 244.5 247.5 244.5 239 239 233.5 250 244.5 241.5 247 239 244.5
```



### transposing data
the Data so far has an awkward structure, with one month per row. The following chunk puts all data from one year to one line, and also starts the "year" in july.

```{r transform_matrix}
converted<-converted[,-35] #that was the "days" column which gets in the way
slist<-unique(converted$ID)

for(i in 1:length(slist)){
station<-filter(converted,ID==slist[i])

#gather-spread combo to have 1 row per year, not 1 row per month

#but first some cleaning:
#if 1 month is missing in one year, it will be filled with NA
#however, if 1 month is missing in all years, it will not be included. This causes major trouble, because it makes the #columns smaller for some stations

#dummy rows with NA need to be included for those months:
if (n_distinct(station$month) < 12){
  m<-mymonths[!(mymonths %in% distinct(station,month)[,1])] #months that are not  included in data frame station
  for (j in 1:length(m)){
    bind<-data.frame(slist[i],distinct(station,year),m[j])
    bind[,4:34]<-NA
    names(bind)<-c("ID","year","month",1:31)
    station<-rbind(station,bind) #now there is one additional column per year and missing month, filled with NA
  }

}


station<-
  station %>% 
  gather(key="day",value="temperature",-(1:3)) %>% #makes one very long dataframe with 2 columns
  mutate (day=str_pad(day,2,pad="0")) %>%  #this makes leading zeros where they are needed
  mutate (temperature=as.numeric(temperature)) %>%
  arrange (year,month,day) %>%
  unite_("un",c("month","day")) %>%  #makes new col with new ID
  spread(key="un",value="temperature")#puts in new wide format with 365 columns (month-day)

station$ID<-slist[i]
write_delim(station,"temp.txt",append=T)
}
```


## calculation mean winter, sd(winter)  

### preparation 

Reload the dataset and remove dates like 31 feb
```{r reload}
rm(station)
rm(converted)
rm(slist)
m<-c("Jul","Aug","Sep","Oct","Nov","Dec","Jan","Feb","Mar","Apr","May","Jun")
days<-paste(rep(m,each=31),rep(1:31,12),sep="")
converted<-fread("temp.txt",sep =" ",col.names=c("ID","year",days))
converted<-data.frame(converted)
x<-NA
for(t in 1:ncol(converted)){x[t]<-sum(!is.na(converted[,t]))}
station<-dplyr::select(converted,which(x>0))
rm(converted)
```



### 1. calculate mean winter onset, sd(winter), predictability  
In the following chunk, a loop selects 1 station at a time. The temperature matrix (number years * 365) is transformed to a binomial matrix (1 if winter is cold enough, 0 if not), and cumSum is used to determine the 5th day of the year that crosses the threshold. The mean winter onset and the standard deviation among years is recorded. Lastly, the 30 days before winter onset of each year are selected, a correlation of temperature ~ time is made, and the slope for each year recorded. The standard deviation between years is another predictability estimate. In the end, all of these variables are saved. A loop is constructed around these calculations to do different thresholds.
  
```{r mean}   
threshlist <-seq(0,150,5)
#t2list <-c(5,10,15)
t2list<-5
reslen<-length(unique(station$ID))

for(threloop in 1:length(threshlist)){
  threshold<-threshlist[threloop]
  for(t2loop in 1:length(t2list)){
    t2<-t2list[t2loop]



#vectors to store results
mean_winter<-rep(NA,reslen)
sd_winter<- rep(NA,reslen)
nyears<-rep(NA,reslen)
ndays_300<-rep(NA,reslen)
ndays_350<-rep(NA,reslen)
ndays_365<-rep(NA,reslen)
p<-rep(NA,reslen)

stations<-unique(station$ID)
for (i in 1:length(stations)){ #this loop goes through each of the stations


  ####read in station####
  id<-stations[i]
  s<-filter(station,ID==id)
  ndays<-rowSums(!is.na(s))-2
  #format of the data: year 1, day 1...365
  #                    year 2, day 1...365
  
   
  
  ####calculate winter arrival#####
 # make a binomial matrix with all Temps<threshold = 1, all others =0(including NA)
  bino<-s[,3:368]<threshold #T/F and hence 0 or 1
  bino[is.na(bino)]<-0
  used<-rowSums(bino,na.rm=T)>t2
  
  #exclude stations that reach winter in less than 3 years 
  if (sum(used) <3) {
    mean_winter[i]<-NA
    sd_winter[i]<-NA
     next
  }
  
  bino<-bino[used,]
  s<-s[used,]
  w_on<-rep(NA,nrow(bino))
  ndays<-ndays[used]
  
  #calcualte row-wise cumulative sums
  #function cumsum works only on columns, so matrix needs transposing
  bino <- 
    bino %>%
    t %>% 
    as.data.frame %>%
    cumsum %>%
    t %>%
    as.data.frame
  
  bino<-bino==t2 #turns it into TRUE/FALSE
 w_on<- max.col(bino, ties.method="first")#first "true" ="threshold reached" in each year
 nyears[i] <- length(w_on)
  
  ##calculate mean and sd in winter arrival (both weighted)####
 
  mean_winter[i]<-weighted.mean(w_on,ndays)
  weights<-ndays/366
  upper <- sum(weights*(w_on-mean_winter[i])^2)
  lower <- sum(weights)-1
  sd_winter[i] <- sqrt(upper/lower)
  ndays_300[i]<-length(ndays[ndays>300])
  ndays_350[i]<-length(ndays[ndays>350])
  ndays_365[i]<-length(ndays[ndays>365])
  
  #clean up memory####
  rm(upper)
  rm(lower)
  rm(weights)
  rm(ndays)
  rm(bino)
  
  ##calculate standard deviation in slopes of last 30 days before winter onset####
  #idea: transpose and use gather to make 1 long vector ("day1:day 10,000" only that they are not   called like that)
        #cut at (winter onset - 30) and winter onset of each year (multiple pieces)
        #clean incomplete data
        #make correlations of temperature ~ time to get the slope of each year
        #calculate sd of the slopes between years
  
  #transpose
  
 rownames(s)<-1:nrow(s) #these are years 1:x
  n<-gather(as.data.frame(t(s[3:368]))) #turns into long format with 2 columns:
  #key = 1,1,1...(366 times), 2,2,2...(366 times),..,40,40... (assuming there are 40 years)
  #value = temperature year 1, day 1:366, year 2, day 1.... 
  n$xvec<-rep(1:(ncol(s)-2),length(unique(n$key))) #xvec is days after 1st july, so it repeats 1:366, 40 times
  n$key<-as.integer(n$key)
  
  #cut
  n<-filter(n,xvec<w_on[key] & xvec >= w_on[key]-31)  #now dataset contains only last 31 days before winter onset in that respective year
  
  #clean
  n2<-group_by(n,key)
  sm<-summarise(n2,sum(!is.na(value))>10) #makes a tibble with 2 cols, key and usable==T or F
  usable<-pull(sm[sm[,2]==T,1]) #makes a vector with all years that have >10 days
  n<-n[n$key %in% usable,]

  #slope estimates
  slopes<-rep(NA,length(usable))
  for (keygroup in usable){ 
    y<-filter(n,key == keygroup)
    slopes[keygroup]<-coef(lm(y$value~y$xvec))[2]
  }
  #translation: for each year take data of that year, save slope estimate from lm
  p[i]<-sd(slopes,na.rm=T)

  } #next station


#####gather and save results####
reslist<-data.frame(unique(station$ID), mean_winter ,sd_winter, p, nyears, ndays_300, ndays_350, ndays_365)

names(reslist)<-c("ID","meanwinter","sd_winter","p","nyears","ndays_300","ndays_350","ndays_365")
write.table(reslist,paste("results_",threshold,"-",t2,".txt",sep=""),append=TRUE,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

```

### calculation colour of noise

In this last part, the colour of noise is calculated. 
```{r}
beta<-rep(NA,n_distinct(station$ID))
for ( i in 1:n_distinct(station$ID)){

  obs<-filter(station,ID == unique(station$ID)[i])
  tr<-as.data.frame(t(obs[3:368]))
  colnames(tr)<-1:ncol(tr)
  obs<-gather(tr)[,2] 
  obs<-data.frame(obs)
  ys           <- nrow(obs)/366

  # fill in gaps by interpolation
  obs          <-na.interpolation(obs)

  # append day
  obs           <- cbind(obs,rep(1:366,ys))
  colnames(obs) <- c("measurement","day")

  # extract daily mean from each observations  
  daily.average <- ave(obs$measurement,obs$day)
  obs$norm      <- obs$measurement - daily.average

  # spectral density
  dens          <- spectrum(obs$norm,plot=F,method="pgram")
  #  select the interval [1/n,.5/month] (i.e. ignore frequencies below the   resolution of the data and above a 2-month period)
  freq          <- dens$freq[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]
  spec          <-dens$spec[dens$freq >= 1/(60*ys) & dens$freq<= 1/59]

  # linear regression
  if(length(freq)<5){next
  }else{
  reg <- lm(log10(spec)~log10(freq))
  beta[i]<--coef(reg)[2]
}

}
df<-data.frame(unique(station$ID),beta)
names(df)<-c("ID","beta")
write.table(df,"beta.txt",quote=FALSE,row.names=FALSE)
```


